<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jaeheon-lee486.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jaeheon-lee486.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-24T06:55:37+00:00</updated><id>https://jaeheon-lee486.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">[Paper Review] Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need</title><link href="https://jaeheon-lee486.github.io/blog/2023/paper-review-rethinking-multiple-instance-learning-for-whole-slide-image-classification-a-good-instance-classifier-is-all-you-need/" rel="alternate" type="text/html" title="[Paper Review] Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need"/><published>2023-07-24T03:15:59+00:00</published><updated>2023-07-24T03:15:59+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/paper-review-rethinking-multiple-instance-learning-for-whole-slide-image-classification-a-good-instance-classifier-is-all-you-need</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/paper-review-rethinking-multiple-instance-learning-for-whole-slide-image-classification-a-good-instance-classifier-is-all-you-need/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need contrastive learning 과 prototype learning, joint training strategy 를 사용하여 기존 MIL 의 문제점을 완화한 연구가 공개되었다. 최근에는 attention pooling 을 활용한 aggregation 기법들이 많이 사용되었는데, attention score 가 굉장히 일부 instance에 대해서만 높아 gradient가 제한적으로 흐르기 때문에 좀 더 괜찮은 instance classifier 를 만들어 mean pooling 하는 전략을 사용하였다. Introduction 1) Low performance in instance-level classificaion : only the most easily identifiable positive instances are found while other more difficult ones are missed : instance pseudo labels contain a lot of noise 2) Bag-level classification performance is not robust : A typical example is the bias that occurs in classifying bags with a large number of difficult positive instances while very few easy positive instances The main contributions of this paper are as follows: We propose INS, an instance-based MIL framework that combines contrastive learning and prototype learning. This framework serves as an efficient instance classifier, capable of effectively addressing instance-level and bag-level classifi- cation tasks at the finest-grained instance level. We propose instance-level weakly supervised contrastive learning (IWSCL) for the first time in the MIL setting to learn good feature representations for each instance. We also propose the Prototype-based Pseudo Label Generation (PPLG) strategy, which generates high-quality pseudo labels for each instance through prototype learning. We further propose a joint training strategy for IWSCL, PPLG, and the instance classifier. We comprehensively evaluated the performance of INS on six tasks of four datasets. Extensive experiments and visualization results demonstrate that INS achieves the best performance of instance and bag classification. Related work Instance-based MIL Methods 주로 각 instance 에 pseudo label 을 붙이고 instance classifier 를 학습하고, bag 내부의 모든 instance prediction 을 aggregate하여 bag classification 을 수행한다. 하지만 positive bag 의 상당 수의 negative instance 가 positive label 관련 noise 를 받게 된다. Bag-based MIL Methods instance feature 를 추출하고 aggregate 하여 bag feature 로 bag classification 을 수행한다. 특히 attention-based method 가 main stream 이고, 독립된 scoring network 의 learnable attention weight 를 사용하여 각 instance feature 점수를 매긴다. 하지만 쉬운 instance 에는 attention score 가 잘 반영되는데, difficult instance 에는 그렇지 않다. Prototype Learning for WSI classification derived from Nearest Mean Classifier, concise representation for instance (내게는) 친숙하지 않은 개념이라 레퍼된 논문을 좀 살펴봤다. TPMIL (TPMIL: Trainable Prototype Enhanced Multiple Instance Learning for Whole Slide Image Classification) create learnable prototype vectors 후, attention score 로부터 얻은 soft pseudo label 과 KL divergence 를 계산하여 update 하는 프레임워크 이다. 하지만 attention score 는 challenging positive instance 를 식별하지 못하는 문제점이 있다. 이를 개선하기 위해, PPLG (prototype-based pseudo label generation) strategy 를 취해, high-quality pseudo-label 을 생성하고, instance contrastive representation learning, prototype learning, instance classifier 간의 joint training 을 수행한다. 특히 negative bags 에서 sampling 한 instance 는 true negative instance 인 점을 활용하여 instance classifier 로부터 prototype learing 에 가이드를 준다. Methods Problem Formulation binary setting 이다. 이 setting 에서는 negative bags 의 모든 instance 는 negative instance 이다. Framework Overview bag 으로부터 instance $x_{i,j}$ 를 sampling 하고 query view 와 key view 를 two augmentation 으로부터 얻는다. query view branch 에서 encoder 를 거친 feature 를 instane classifier 와 projector (MLP-based) 에 넣고, 각각으로부터 predicted class ${\hat{y}{i,j} \in R^2}$, feature embedding ${q{i,j}\in R^d}$ 를 얻는다. key view branch 에는 gradient 가 흐르지 않는 대신, query branch 로부터 momentum update 방식으로 모델이 업데이트 된다. 그림처럼 encoder 와 projector 를 거쳐 ${k_{i,j}}$ 를 얻는다. 여기서 조금 복잡한데, 위 query branch 로부터 얻었던 predicted class ${\hat{y}{i,j} \in R^2}$ 와 함께 넣어주는 것이 포인트이다. 이를 embedding queue 에 enqueue 한 후, 내부에서 contrastive learning loss 를 계산한다. 이 부분은 아래 다른 섹션에서 더 설명하도록 하겠다. 마지막으로, 다시 query branch 로부터 나온 ${\hat{y}{i,j} \in R^2}$ 와, prototype vector와 loss 를 계산한다. PPLG module 내에서는 two representative feature vector 를 positive, negative class training 과정에서 계속해서 업데이트 한다. 이 또한 아래 섹션에서 더 설명하도록 하겠다. Instance-level Weakly Supervised Contrastive Learning 목적: 좋은 feature representation 을 얻는 것. 논문에서도 언급하지만, 사실 contrastive learning 에서 가장 중요한 것은, 식이 어떻고를 떠나서 어떻게 negative, positive sample set 을 construction 하느냐가 핵심이다. 기존 self-supervised learning setting 에서는 본인과 본인을 제외한 나머지로 보통 셋을 구성했지만, MIL setting 에서는 negative bag in the training set 으로부터 뽑은 all instance 는 true negative label 을 갖고 같은 셋에 속하도록 할 수 있다. 이러한 트릭을 다른 어느 논문에서도 사용하지 않았다고 한다. 이 weak label 정보는 instance-level contrastive learning 을 효율적으로 guide할 수 있었다고 한다. 특히 large Embedding Queue 에서 feature embedding $k_{i,j}$ 와 그것들의 class $\hat{y}_{i,j}$ 를 함께 저장했고, 이 때, true negative instance 같은 경우에는, predicted class 를 저장하지 않고, 바로 directly assign them a definite negative class 로 하였다. 이 부분을 계속 강조하는 이유는,, 나중에 framework 를 좀 더 자세히 봤을 때, &#39;임의로 만든 것&#39; 과 &#39;임의로 만든 것&#39; 간의 거리를 비교하고 또 &#39;임의로&#39; 만드는 과정이 들어가는데, 그것의 base... 기반이 되어주는 true basic label 으로써 굉장히 중요한 역할을 수행한다. Family and Non-family sample selection large embedding queue 에서 family set $F(q_{i,j})$ 와 non family set $F&#39;(q_{i,j})$ 를 구성하고, $q_{i,j}$ 에 기반하여 contrastive loss 를 계산한다. 이 때, $F(q_{i,j})$ 는 q, k 와 embedding queue whose class label equals $\hat{y}{i,j}$ 로부터 구성되고, 그 나머지는 모두 $F&#39;(q{i,j})$ 가 된다. 이것이 전부이다. B 는 minibatch 를 뜻한다. Q는 embedding queue 이다. 위 family set 과 non-family set 을 이용해 다음과 같은 loss 를 계산한다. Embedding Queue Updating iteration 끝마다, current instance&#39;s momentum embedding $k_{i,j}$ 와 predicted label 또는 true negative label 은 embedding queue Q 에 들어가고 오래된 건 dequeue 된다. Prototype-based Pseudo Label Generation 목적: assign more accurate pseudo labels to instances by prototype learning 아직 설명 전이지만, 이 과정을 통해, maintain two representative feature vector 를 얻는다. $\mu_r \in R^d$, $r=0,1$. 사실 pseudo label 의 생성과 prototype 의 updating process 역시 (아까 말했듯) true negative instance 와 instance classifier (predicted label을 만들어 주기 때문) 에 의해 guide 된다. 만약 $x_{i,j}$ 가 positive bag 으로 부터 왔다면, embedding $q_{i,j}$ 과 prototype vectors $\mu_{r}$ 로 pseudo label $s_{i,j}\in R^2$ 를 생성한다. 동시에, prototype vector $\mu_{?}$ of the corresponding class 를 predicted label 과 embedding q 로 update 한다. 만약 negative bag 으로부터 왔다면, directly assign negative label 후, embedding q 를 활용하여 negative prototype vector 를 update 한다. 마지막으로, generated pseudo label 을 실제 encoder 로부터 나온 smoothed predicted value $p_{i,j} \in R^2$ 간의 cross-entropy loss 를 계산하여 iteration 을 마치게 된다. Bag Constraint and Total Loss 정말 마지막 단계로, bag label 을 조금 더 활용하기 위해 embedding vector $q_{i,j}$ 를 mean pooling 해서 bag label 과 cross entropy loss 를 계산해서 total loss 에 더해준다. 요즘 트렌드인 attention weight 를 전혀 사용하지 않은 부분이 상당히 인상적이다. Experimental Settings Dataset Simulated CIFAR-MIL Dataset : 32x32 이미지 합쳐서 WSI 처럼 합성, 하나의 category label 을 positive 로 정의. Camelyon16 Public Dataset : 512x512 image patches under 10x mag, total 186604 instances. TCGA LUNG Cancer Dataset : 1054 WSIs, 5.2 million patches at 20x mag. Cervical Cancer Dataset : 374 WSIs, 5x mag, 224x224 patches Evaluation Metrics and Comparision Methods in both instance and bag classification, AUC &amp; acc are used. compared out INS to 11 competitors: MILRNN, Chi-MIL, DGMIL (instance) ABMIL, Loss-ABMIL, CLAM, DSMIL, TransMIL, DTFD-MIL, TPMIL, WENO (bag) Results Synthetic Dataset CIFAR-MIL Camelyon 16 Dataset TCGA-LUNG cancer Dataset Cervical Cancer Dataset Interpretability Study of the Lymph Node Metastasis INS to predict the probability of each instance being positive within the positive bags and visuazualied the top 0.1% instances with the highest and lowest probilities separately. lymph node metastasis &quot;micropapillae&quot; are more prevalent 특히, small clusters of infiltrating cancer cells forming hollow or mulberry-like nests without a central fibrovascular axis, surrounded by blank lacunae or lacunae between interstitial components negative lymph node more commonly exhibits a sheet-like pattern 나머지가 조금 있긴 한데 추후에 업데이트 하겠다.]]></summary></entry><entry><title type="html">a post with bibliography</title><link href="https://jaeheon-lee486.github.io/blog/2023/post-bibliography/" rel="alternate" type="text/html" title="a post with bibliography"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/post-bibliography</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/post-bibliography/"><![CDATA[<p>This post shows how to add bibliography to simple blog posts. If you would like something more academic, check the <a href="/blog/2021/distill/">distill style post</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[an example of a blog post with bibliography]]></summary></entry><entry><title type="html">a post with jupyter notebook</title><link href="https://jaeheon-lee486.github.io/blog/2023/jupyter-notebook/" rel="alternate" type="text/html" title="a post with jupyter notebook"/><published>2023-07-04T12:57:00+00:00</published><updated>2023-07-04T12:57:00+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/jupyter-notebook</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/jupyter-notebook/"><![CDATA[<p>To include a jupyter notebook in a post, you can use the following code:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{::nomarkdown}
{% assign jupyter_path = "assets/jupyter/blog.ipynb" | relative_url %}
{% capture notebook_exists %}{% file_exists assets/jupyter/blog.ipynb %}{% endcapture %}
{% if notebook_exists == "true" %}
    {% jupyter_notebook jupyter_path %}
{% else %}
    <span class="nt">&lt;p&gt;</span>Sorry, the notebook you are looking for does not exist.<span class="nt">&lt;/p&gt;</span>
{% endif %}
{:/nomarkdown}
</code></pre></div></div> <p>Let’s break it down: this is possible thanks to <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll Jupyter Notebook plugin</a> that allows you to embed jupyter notebooks in your posts. It basically calls <a href="https://nbconvert.readthedocs.io/en/latest/usage.html#convert-html"><code class="language-plaintext highlighter-rouge">jupyter nbconvert --to html</code></a> to convert the notebook to an html page and then includes it in the post. Since <a href="https://jekyllrb.com/docs/configuration/markdown/">Kramdown</a> is the default Markdown renderer for Jekyll, we need to surround the call to the plugin with the <a href="https://kramdown.gettalong.org/syntax.html#extensions">::nomarkdown</a> tag so that it stops processing this part with Kramdown and outputs the content as-is.</p> <p>The plugin takes as input the path to the notebook, but it assumes the file exists. If you want to check if the file exists before calling the plugin, you can use the <code class="language-plaintext highlighter-rouge">file_exists</code> filter. This avoids getting a 404 error from the plugin and ending up displaying the main page inside of it instead. If the file does not exist, you can output a message to the user. The code displayed above outputs the following:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/blog.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Note that the jupyter notebook supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="jupyter"/><summary type="html"><![CDATA[an example of a blog post with jupyter notebook]]></summary></entry><entry><title type="html">[Paper Review] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title><link href="https://jaeheon-lee486.github.io/blog/2023/paper-review-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture/" rel="alternate" type="text/html" title="[Paper Review] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"/><published>2023-06-21T08:06:37+00:00</published><updated>2023-06-21T08:06:37+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/paper-review-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/paper-review-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture SNS 피드를 보다가 I-JEPA 관련 뉴스를 봤다. 제목 자체가 굉장히 intriguing 했는데 https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa/ &#39;I-JEPA: The first AI model based on Yann LeCun&#39;s vision for more human-like AI&#39; 였다.. 게재된지는 꽤 되었지만 이번 CVPR 2023 발표와 함께 다시 한번 수면으로 올라왔다. paper 의 핵심 architecture 를 이해하면서 human-like AI 라는 term 에 대해 공감이 갔는데, 이유는 다음과 같다. 기존 generative approach for SSL 방식에서는 reconstruction 을 한 후에 loss 를 계산했는데, I-JEPA 에서는 representation space 에서 reconstruction (?) 및 loss 계산이 이루어진다. augmentation 이 따로 필요없다는 점도 포함될 수 있다. Introduction computer vision field 에서 two common families of approches for self-supervised learning 방식을 소개한다. Invariance-based pretraining methods (MoCo, SimSiam, VICReg, BarlowTwins...) Generative methods (MAE, language models) 먼저 1번 방식의 문제점을 언급한다. pretraining methods can produce high semantic representations but introduce strong biases that may be detremental for certain downstream tasks. unclear how to generalize these biases for tasks requiring different levels of abstraction. (ex, segmentation &amp; classification do not require the same invariances.) (끄덕끄덕) 또한 필자는 Cognitive learning 이론에서는 biological system 에서의 representation learning 의 핵심은 &quot;the adaptation of an internal model to predict sensory input responses&quot; 이다. 라고 하며 core of generative SSL model 의 핵심인 &quot;remove or corrupt portions of the input and learn to predict the corrupted content&quot; 와 같은 선상에 있다고 주장한다. 이어서 이 방식의 문제점도 함께 언급한다. lower sementic level 의 representation 을 도출하고, 종종 underperform invariance-based pretraining inf off-the-shelf evaluation (ex linear probing) 하다고 지적한다. 본 논문에서는 SSL representation 의 semantic level 을 prior knowledge (encoded through augmentatino) 없이 개선할 수 있는가에 대한 고민을 담고 있고, 이를 &quot;predict missing information in ab abstract representation space&quot; 하는 것으로 좋은 성능을 만들었다고 한다. 다음은 introduction 맨 마지막에 있는 we demonstrate that ~ 이다. I-JEPA learns strong off-the-shelf representation without the use of hand-crafted view augmentations. I-JEPA outperforms pixel-reconstruction methods (like MAE) on ImageNet01K linear probing, etc I-JEPA 는 view-invariant pretraining model 과 semantic task 에서 competitive 했고, low-level vision task 를 outperform 함. (applicable to a wider set of tasks) I-JEPA is also scalable and efficient Background 보통 background 는 간단히 하고 넘어가는데 읽는 것만으로 도움이 많이 되는 내용들이라 자세히 작성하려 한다. Self-supervised learning 은 &quot;inputs 들 간의 relationship 을 capture&quot; 하는 학습 방식이다. 이는 framework of Energy-Based Models (EBM) 으로 접근할 수 있다. incompatible input 에 대해서는 high energy를, compatible inputs 에 대해서는 low energy 를 할당하는 것이다. 현존하는 generative 및 non-generative model 에서는 아래 세가지 framework 로 설명할 수 있다. Joint Embedding Architectures JEA 는 비슷한 input 에 대해 비슷한 embedding, 다른 input 에 대해 서로 다른 embedding 을 만들도록 학습한다. Main Challenge with JEA 는 &quot;representaion collapse&quot; (aka dimensional collapse) 이다. energy landscape 가 flat 한 경우, 즉 encoder 가 trivial embedding 을 만드는 현상이다. 이러한 dimensional collapse 를 막기 위해 다음과 같은 loss 들이 연구되었었다. contrastive losses : push apart embeddings of negative examples non-contrastive losses : minimize informational redundancy clustering-based approches : maximize the entropy of the average embedding 그림의 (a)에 해당하는 구조이다. representation space 에서 보통 loss가 계산된다. (전에도 리뷰한 적이 있는 내용인데) dimensional collapse 를 피하는 가장 핵심은 leverage an assymmetric architectural design between x-encoder and y-encoder 하는 것이었다. Generative Architectures 여태 리뷰했던 논문들은 대부분 JEA 였고 생성 기반 구조를 가진 논문은 MAE 정도만 읽어본 것 같다. (b) 내용인데, 이 구조는 additional variable z (cGAN (SSL은 아니지만) 에서의 condition에 해당할 수도 있고 MAE 의 mask 에 해당하는 부분 또는 position token 일 수도 있다) condition 위에서, directly reconstruct a signal y from a compatible signal x 이고, loss 는 reconstructed signal 과 원래 있던 기존 signal 사이에서 계산된다. Joint-Embedding Predictive Architectures I-JEPA 에서 제안하는 구조이다. (c) 내용이고, (b) 구조와 상당히 닮아 있으면서, loss function is applied in embedding space (not input space) 라는 점이 key difference 이다. 또한 hand-crafted augmentation 을 없애는 대신, representation 상에서의 &quot;mask&quot;, 즉 additional information &quot;z&quot; 를 달리하여 condition된 representation 을 predict 한다. 물론 representation space 상에서의 작업이기 때문에 dimensional collapse 를 염두에 두고 asymmetric architecture 를 설정하는 것도 중요함을 강조했다. Method now describe the proposed Image-based Joint-Embedding Predictive Architecture (I-JEPA) context-encoder 와 target-encoder, 그리고 predictor (가 아직 뭔지 모르지만) 에서 ViT architecture 를 사용했다. 아래 두 줄로 Overall objective 를 설명할 수 있다. given a context block predict the representations of various target blocks (in the same image) 뭔가 MAE 와 비슷해보인다. 그럼 context block 과 target block 은 뭘까 라는 질문이 든다. 그 전에 target 이 뭔지부터 규명한다. Targets how we produce the targets? 우선 y 라는 image 를, N non-overlapping patches 를 생성한다. 이후 이를 target-encoder $f_{\hat{\theta}}$ 에 집어넣고, patch-level representation $s_y$ = {$s_{y_1},,, s_{y_N}$} 을 얻는다. 그리고 여기서 loss 에 사용할 M 개의 block 을 뽑는데, 1 개의 block 마다 여러개의 patch 가 들어있고 이는 overlap 이 가능하다. 이 block 을 지칭하는 &quot;mask&quot; 하나를 $B_i$ 라고 부르고, 그에 해당하는 i번째 block 의 patch set 을 $s_y(i) = {S_{y_j}}_{j\in B{i}}$ 라고 부른다. 보통 M 은 4고 aspect ratio 는 (아마 너비&amp;높이) (0.75, 1.5) 이고, random scale (아마 크기) 는 (0.15,0.2) 이다. 저자는 &quot;target block&quot; 은 masking the &quot;output of the target-encoder&quot; 이지 not &quot;input&quot; 이라고 강조한다. 즉 input 에 바로 mask 씌우는 것이 아니라 embedding 된 것에 씌우는 것이라고 강조한다. Contexts Recall, the goal behind I-JEPA is to predict the target block representations from single context block. image 에서 random scale (0.85, 1.0) 으로 그리고 unit aspect ratio 로 a single block x 를 뽑고 이 mask 를 $B_x$ 라고 부른다. target block 과 context block 모두 random 하게 뽑았으니 당연히 겹칠 수 있다. 위 그림은 target block 과 context block 의 예시이다. 마찬가지로 context block 역시 context encoder $f_\theta$ 에 넣어 $s_x = {S_{x_j}}_{j\in B{x}}$ 를 만든다. 위의 target encoder 와는 독립된 네트워크이다. Prediction 이제 $s_x$도 뽑았고 M개의 target block 에 해당하는 patch representation set인 $s_y(1), ... , s_y(M)$ 도 뽑았다. 이 때, 주어진 각 $s_y(i)$에 대해, predictor $g_\phi(-,-)$ 는 context encoder 의 output 인 $s_x$ 와 mask token for each patch ${m_j}_{j\in B_i}$ 를 받아서 $\hat{s}_y(i)$ 를 만든다. mask token is parameterized by a shared learnable vector with an added positional embedding. 즉, mask 는 constant 가 아니라 parameter 이다. 처음에는 이 부분이 굉장히 헷갈렸다. 위 그림을 보면, context block 과 target block 이 전혀 겹치지 않고 있는데 어떻게 masking 을 한다는거지? 라는 의문이 들어 헷갈렸다. 하지만 물리적인 masking 을 의미하는 것이 아니라, 하나의 parameter z 즉 condition (너는 이 부분을 예측해야 해 라는 정보) 로 받아 들이고 나니 이해가 수월했다. 그냥 concat이다. context block patch representation 과 mask token, positional encoding 을 concat, add 해서 predictor network 에 통과시키고, 이를 target block patch representation 과 비교하는 것이다. Loss L2 loss 를 사용했다. Image Classification I-JEPA 가 좋은 high-level representation 을 augmentation 없이도 잘 학습한다는 것을 보이기 위해, linear probing 과 partial fine-tuning protocol 을 통한 image classificaion task 를 수행하였다. ImageNet-1K dataset 으로부터 pretrained 된 다른 self-supervised model 을 비교군으로 사용했고, 모든 I-JEPA 모델은 resolution 224 x 224 pixels 로 train 되었다. ImageNet-1K Table1. 은 common ImageNet-1K linear-evaluation benchmark 에 따른 성능 기록이다. SSL 사전학습 후 frozen 된 후 linear classifier 가 학습되었다. data augmentation 없는 MAE와 CAE (context autoencoder), data2vec 이 비교 실험 모델로 사용되었다. augmentation 이 없었음에도 불구하고 invariant approach (like iBOT) 과 competitive 한 성능을 보여주었다. Low-shot ImageNet-1K Table2 는 1% ImageNet benchmark performance 이다. 1%의 available ImageNet label 을 사용하였다. I-JEPA 는 MAE 보다 less pretraining epoch 을 사용하여 outperform 하였고, data2vec 보다 더 작은 네트워크를 사용하여 비슷한 성능을 내었다. input resolution 을 높여 (to 448) 학습했을 때 이전 data augmentation 을 사용했어야만 했던 invariance approach SSL 방식들의 결과를 모두 뛰어넘었다. 흠.. Transfer learning Local Prediction Tasks 위에선 classification task 에 대해 확인했는데, 기존 generative 성능을 뛰어넘을 뿐만 아니라 high semantic task 에 특화된 invariance based method 에도 competitive 한 성능을 기록했다. 이제 local image features 도 잘 학습하는지를 알아보았다. object counting과 depth prediction 에서 view-invariance based method 를 outperform 했다. Scalability Model Efficiency I-JEPA 는 기존 방법보다 highly scalable 함을 보이고 있다. 이전 방식들보다 더 적은 시간으로 더 높은 성능을 기록하고 있다. (task: semi-supervised evaluation on 1% ImageNet-1K) Scaling data size &amp; model size I-JEPA 는 larger dataset 에서 pretraining 될수록 benefit 이 있다는 것을 확인했고, larger model size 로부터 학습했을 때 더 높은 성능을 기록함을 확인하였다. (scalability) Predictor Visualization I-JEPA에서 predictor의 역할은, predictor에 context encoder 의 output 과 함께, positional mask token 을 통과시켰을 때 target block representation 을 예측하도록 하는 것이다. 이것이 representation space level 에서 이루어지기 때문에 이를 visualization 해보았다. 캬 너무 멋지다 방법은 RCDM framework 에 따라 decoder 를 학습했다고 하는데, 정확히 이해는 못했다. (논문 reference 를 참고하도록 하자) Ablations Predicting in representation space I-JEPA 의 핵심 중 하나는 representation space 에서 loss 를 계산한다는 것이다. 이것이 효용이 있는지 확인하기 위해 pixel-space 와 representation-space 에서 loss 계산해서 성능을 비교하였다. Masking strategy mask 방식을 달리하여 실험을 여럿 돌렸다. Conclusion 재밌구만..]]></summary></entry><entry><title type="html">[Paper Review] RetCCL: Clustering-guided contrastive learning for whole-slide image retrieval</title><link href="https://jaeheon-lee486.github.io/blog/2023/paper-review-retccl-clustering-guided-contrastive-learning-for-whole-slide-image-retrieval/" rel="alternate" type="text/html" title="[Paper Review] RetCCL: Clustering-guided contrastive learning for whole-slide image retrieval"/><published>2023-05-19T05:38:13+00:00</published><updated>2023-05-19T05:38:13+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/paper-review-retccl-clustering-guided-contrastive-learning-for-whole-slide-image-retrieval</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/paper-review-retccl-clustering-guided-contrastive-learning-for-whole-slide-image-retrieval/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[RetCCL: Clustering-guided contrastive learning for whole-slide image retrieval 디지털 병리 AI 심포지움에서 reference 되었던 breast npj 2023 논문에서 차용한 SSL 방법이다. WSI 에서 SSL 방식을 도입할 때 발생할 수 있는 문제점을 인지하고 이를 해결하기 위해 fancy 한 방법을 사용했다. 결과 좋은 성능을 내었고 많은 논문에서 인용되고 있어 의미 있는 논문이라 생각한다. Introduction WSI annotation 및 computation cost 높음. SSL 방식을 사용하면 이 문제를 alleviate 할 수 있음. 기존 SimCLR 등의 방식에서 patch 를 instance 로 여겨 학습하면, serious bias 생김. unbalanced tissue type distribution 와 large portion of similar tissues 이를 위해 다음 두가지 방법을 사용한 framework 를 제안함. 1) clustering-guided contrastive learning (CCL) for feature extraction 2) distinctive query patch selection, ranking for searched pathes, and aggregation algorithm for interpretable WSI searching Related works Self-supervised representation learning natural image application to the histopathology domain : domain shift focus on semantic features within the specific task : limited model generalization not tested on large and diverse histopathological image datasets Histopathological image retrieval traditional hand-craft features DL method based high-level features suboptimal performance due to the domain shift btw natural and histopathological images Methods 다음과 같은 구조로 되어있다. weighted InfoNCE, group-level InfoNCE 활용한 feature extraction 1) offline database construction for WSI retrieval 2) online WSI query process Contrastive learning based feature extractor Preliminary of contrastive learning x: image, x_q, x_k: augmented views 라 할 때, contrastive learning 은 q &amp; k+ sample 을 가까이에, q &amp; k- sample 을 멀리 두도록 학습한다. 가장 기본적인 loss 는 다음과 같이 쓸 수 있다. sigma L 이 포함된 묶음은 memory bank 내 negative samples 을 의미한다. 이를 통해 momentum encoder 이 moving average 방식으로 update 된다. proposed clustering-guided contrastive learning 위와 같은 방식을 사용했을 때 다른 데이터셋에선 잘 나올지 몰라도 WSI 데이터에는 맞지 않을 수 있다. 위 식으로 봤을 때, negative sample memory bank에 positive sample 로 consider 되어야 할 highly correlated sample 이 포함되어 있을 가능성이 높다. 또, different patch 들이 같은 tissue 로부터 샘플링 되었고 strikingly similar appearance 를 가졌다면 문제는 더 심각해진다. 이는 histopathology 분야에서 SSL 방식을 도입할 때 줄곧 언급되어 왔던 한계점이기도 하다. 이 문제를 해결하기 위해, 두가지 loss (weighted InfoNCE &amp; group-level InfoNCE) 로 구성된 clustering-based contrastive learning method 를 제안한다. W-InfoNCE loss 는 possible false-negative sample 의 영향을 줄이기 위함이고, G-InfoNCE loss 는 distinctive group center 끼리는 멀도록 하고 다른 instance 는 group 으로 모으는 역할을 한다. 먼저 three augmented images 를 얻는다. (xp, xk, xq) 그리고 두개의 encoder h (xp, xq) 와 f (xk) 를 통과시켜 feature (hp, fk, hq) 를 얻는다. 이후 two MLP heads g1, g2 를 이용해, hp 를 gp1, gp2 로 encoding 하고, hq 를 hq1, hq2 로 encoding 한다. fk 의 경우 g2 를 이용해 gk 를 얻는다. MLP head g2 를 통과시켜 나온 세 개의 feature gp2, gq2, gk 를 이용해 weighted InfoNCE loss 를 계산하게 되고, MLP head g1 을 통과시켜 나온 두 개의 feature gp1, gq1 은 cluster 하는 데에 사용되어 향후 group-level infoNCE loss 에 활용된다. 이후 final loss function 은 두 loss 의 조합으로 계산된다. Online clustering-guided memory bank construction weight 를 주어 false-negative-like sample 의 영향을 줄임으로써 기존 memory bank 와는 차별된 online clustering-guided memory bank 를 제안한다. 매 training epoch 마다, two positive pairs {gp2, gk}, {gq2, gk} 를 활용해, shared memory bank 로 contrastive learning 을 수행한다. 우선 negative sample 을 k-means clusering 을 활용해 Q classes 로 나누고, 이를 Q sub-memory queues 로 본다. 이 때, 각 centroid c (q개) 와 gk 간의 similarity 를 계산하고, 이 similarity 값 중 가장 큰 값을 갖는 Q 를 Q_max 라고 지정한다. 이후 negative sample 과의 similarity 를 계산할 때 다음 $\phi (g_k-)$ 을 씌워서 영향력을 조절한다. 즉 가장 가까운 Q cluster 에 해당하는 negative sample, 즉 false-negative-like sample 의 영향력을 줄이는 것이다. 다음과 같은 식으로 weighted infoNCE loss 를 표현한다. Group-level discrimination 이 식은 2021 CVPR 에 올라왔던 cross-level discrimination (CLD) loss function 으로부터 따왔다. 여기에 auxiliary branch 를 붙여 unbalanced positive/negative sample ratio 를 해소하려 하였다. 다시 위 그림에서, 이번엔 g1 MLP head 를 보자. hp 와 hq 는 다시 g1 을 거쳐 gp1, gp2 vector 를 생성한다. mini-batch 내에 있는 embedding 들은 각 두 branch 에서 S개의 cluster 로 clustering 되고, 각각의 cluster centroid 도 할당한다. 이때 요 cluster centroid 를 이용해서 다시 positive negative sample 을 branch 에서 정의하고 group-level InfoNCE loss 를 계산하게 된다. 구체적으로, g__p__1 (p branch이다) 이 있을 때, 이것의 positive sample 은 S개의 cluster 중 가장 가까운 1개의 cluster centroid ($S^{q+}$, q branch 임) 의 sample 이 되는 것이고, 나머지 S-1 cluster 는 negative sample 이 된다. 식은 다음과 같다. Group-level infoNCE loss 와 위에서 계산했던 weighted InfoNCE loss 의 combination 으로 final loss 를 꼐산하고 update 한다. WSI retrieval method Preliminary of WSI retrieval WSI retrieval의 background 를 알려주고 있긴 한데 우선 나는 WSI retrieval 이 무엇인지도 잘 몰랐다. retrieval은 대략 &quot;검색&quot; 이라는 뜻을 가지고 있는데, 다른 reference 논문을 타고 들어가보니, 다음과 같은 작업을 가리키는 것 같았다. 결국 feature extraction 을 잘 수행하고, similarity 계산을 잘 수행해서 WSI 내에서 빠르게 잘 찾아내는 것이 task 의 핵심이라고 생각하면 될 것 같다. 두가지 방식을 소개하고 있다. Yottixel method (Y) FISH 공통적으로 두 방식은 WSI 를 patch 로 나누고 clustering 을 한 후 query 에서 patch-by-patch matching 후 database 에서 가장 비슷한 patch 를 선정한다고 한다. 이 때 Y 는 pretrained DenseNet 으로 feature를 뽑고, 이를 binary code 로 compression 하는 반면, FISH 는 VQ-VAE pretrained on TCGA 로 generate texture feature 한다. 또한 Y 는 false match 를 줄이기 위해 Hamming distance 를 계산하고, FISH 는 Van Emde Boas tree with an uncertainty-based ranking algorithm 을 사용한다고 한다. Proposed WSI retrieval method overall procedure 는 다음과 같다. 크게 offline database construction, online WSI query process 두가지로 나뉜다. Database Construction for WSI Retrieval offline database construction 에서 먼저 patchify 하고 CCL-based feature extractor 를 거쳐 나온 feature 로 mosaic generation 을 하는데, mosaic generation 은 dual clustering 으로 이루어져 있다. 먼저 CCL-feature-based K-means clustering 을 수행하고, 그 coordinate 를 받아 spatial-coordinate-based clustering algorithm 을 수행한다. 그 결과 distinctive patches (called a mosaic) 이 생성되고 full WSI 를 대표하게 되는 것이다. 역시 말보단 알고리즘이지. FeatureKMeans 함수로 K1 개의 cluster 로 나눈 뒤, 이를 SpatialKMeans 를 거쳐 다시 각 cluster 를 4-5개의 cluster 로 나눈다. 즉, 4-5개 * K1 개의 cluster centroid 가 생기고 이것이 distinctive patch 가 되는 것이다. WSI Query Process WSI database construction 이 완료되고, online 으로 patch-level retrieval 을 시작한다. nearest neighbor searching method 를 사용하고, 이 retrieved patch 와 meta-information 으로 ranking and aggregation algorithm 을 활용해 가장 비슷한 WSI 를 찾아준다. 이 부분도 알고리즘으로 보는게 이해가 잘된다. WSI image query 를 받으면, 각 patch k 개에 대해 feature vector 를 얻고, 이에 대한 retrieval bag 을 k개 얻는다. bag 1개 당 t 개의 retrieved patch 가 있는데 bag 마다 다르다고 한다. bag 1개의 retrieved patch 와 WSI 의 patch 사이의 cosine similarity d 를 구하고, 이 d 와 diagnosis 정보를 활용하여 probability pm 을 계산한다. 이 pm 은 결국 요약해서 말하면, bag 안에서 m번째 diagnosis type 의 확률이다. yj 는 database 로부터 나온 diagnosis information (bag 의 정보) 이다. $\delta()$ 는 두 개의 input 이 같으면 1 을 뱉고 다르면 0을 뱉는 일종의 indicator function 이다. $w_{y_j}$ 는 occurrence frequency (normalized probability) 인데, 이것도 역시 jth diagnosis type 이 얼마나 database 에 있는지를 나타낸다. $(d^j +1)/2$ 는 [-1,1] 인 cosine similarity 의 range 를 [0,1] 로 transform 해준 것이다. 간단히 생각해서 $w_{y_j}(d^j +1)/2$ 자체를 j 번째 sample 에 대한 score 라고 치환해서 생각하면 된다. 그럼 식 자체는 전체 score 의 합 중에서, m 번째 diagnosis subtype 에 포함된 score 의 합이 되어 m 번째 diagnosis type 의 확률로 여길 수 있다. 이 pm 을 이용해서 entropy 를 각 bag 에 대해서 계산하고, ranking 즉 reorder 한다. 이후 procedure REMOVE BAGS WITH LOW QUALITY 에서, AveTop function 을 이용해서 criterion $\eta$ 를 구하는데, AveTop function 은 각 bag 마다 상위 5개의 cosine similarity 를 평균 낸 것이다. 이 criterion 을 넘지 못하는 bag 들은 remove 한다. 이 bag 내부에서 top 5개의 patch 를 고르는 것으로 마무리가 된다. Experimental results and discusions Datasets TCGA PAIP UniToPatho TissueNet DiagSet-A.2. Experimental metric 다음 두가지 (많이 쓰인다고 하는) Acc@k (top-k accuracy) and mMV@k(majority vote at the top k search results) 를 썼다. Acc@k: retrieved similar image 가 query image와 correct label 이면 높게 측정 mMV@k: more strict metric, majority 가 query image 와 correct label 이어야 높게 측정 Results of patch-level retrieval Effect of network components key innovation 인 clustering-based memory bank construction (Mem) 을 추가했을 때와 group-level InfoNCE (Gro) 를 추가했을 때 baseline 보다 성능이 향상되었고 같이 혼합해서 사용했을 때 가장 높았다. 이외에도 다양한 hyperparameter 및 setting 에 대한 abltation study 를 진행했고, 논문에 자세히 언급되어 있다. Comparison between our CCL and other SSL-based feature extractors SimCLR v1, SwAV, Moco v2 과 CCL-based feature extractor 의 retrieval 성능을 비교했다. 성능면에서 측정한 모든 metric 에 대해 가장 좋은 성능을 보여줬고, 특히 가장 strict 한 metric 인 mMV 에서의 성능 향상이 컸다. UniToPatho (normal, hyperplastic polyp, tubular adenoma, tubulo-villous adenoma) 에서 각 subtype 을 nice 하게 잘 찾은 것을 볼 수 있다. 또한 TissueNet의 결과를 봤을 때, texture 와 color 가 많이 다름에도 잘 찾은 것을 볼 수 있다. 그 아래 failed case 가 소개 되어 있는데, 저자는 실패했음에도 morphological feature 가 pathologists 간의 disconcordance 를 야기할 정도로 굉장히 비슷했다고 주장한다. Results of WSI retrieval 1) searching for anatomic sites 2) searching for cancer subtypes based on the same human site Results of anatomic site retrieval 앞선 Yottixel 과 FISH 에서 사용했던 frozen and FFPE WSIs in TCGA 를 사용해서 성능 측정이 이루어졌다. 당연하게도 database 는 모든게 포함된 database 가 아니라 TCGA 3만장 (FFPE 11791, frozen 15237) 이 포함된 database 에서 진행했다고 한다. 결과 20% 넘는 성능향상까지 보여주었다. Results of cancer subtype retrieval 각 frozen 과 FFPE WSI 에 대해서 +10% than Yottixel and +3% than FIST in FFPE, +40% improvement on specific subtype such as MESO 등 큰 성능 향상을 보였다. Interpretability analysis Results of downstream classification task 제안한 SSL pre-trained feature extractor 가 다른 downstream task 에서도 유용하게 사용될 수 있음을 보였다. DiagSet-A.2 dataset 에서 four-class classification task 를 진행했고 그 결과를 나타내었다. highest 를 보였던 SwAV 보다도 성능이 높게 측정되었고, 20% 데이터를 training 에 사용했을 때 ImageNet 이 100% 를 사용했을 때 (supervised learning setting) 보다 성능이 높게 측정되어, SSL 방식의 효용성을 보였다. Conclusion WSI-level, patch-level 에서 사용가능한 histopathological image retrieval algorithm 을 제안했고, visually interpretable result 를 보여주었다. CCL-based backbone model 을 새로 고안했으며, database construction 후 ranking, curation, and aggregation 이 포함된 retrieval algorithm 을 사용했다. 이는 current WSI retrieval method에서 보여주는 성능을 큰 폭으로 뛰어넘었고 feature 는 다른 downstream task 에서 사용될 수 있을만큼의 potential 을 보여주었다. 와 진짜 실험하느라 힘들었겠다]]></summary></entry><entry><title type="html">a post with custom blockquotes</title><link href="https://jaeheon-lee486.github.io/blog/2023/custom-blockquotes/" rel="alternate" type="text/html" title="a post with custom blockquotes"/><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/custom-blockquotes</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/custom-blockquotes/"><![CDATA[<p>This post shows how to add custom styles for blockquotes. Based on <a href="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</a> implementation.</p> <p>We decided to support the same custom blockquotes as in <a href="https://sighingnow.github.io/jekyll-gitbook/jekyll/2022-06-30-tips_warnings_dangers.html">jekyll-gitbook</a>, which are also found in a lot of other sites’ styles. The styles definitions can be found on the <a href="https://github.com/alshedivat/al-folio/blob/master/_sass/_base.scss">_base.scss</a> file, more specifically:</p> <div class="language-scss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Tips, warnings, and dangers */</span>
<span class="nc">.post</span> <span class="nc">.post-content</span> <span class="nt">blockquote</span> <span class="p">{</span>
    <span class="k">&amp;</span><span class="nc">.block-tip</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-warning</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-danger</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>A regular blockquote can be used as following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; This is a regular blockquote</span>
<span class="gt">&gt; and it can be used as usual</span>
</code></pre></div></div> <blockquote> <p>This is a regular blockquote and it can be used as usual</p> </blockquote> <p>These custom styles can be used by adding the specific class to the blockquote, as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### TIP</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; A tip can be used when you want to give advice</span>
<span class="gt">&gt; related to a certain content.</span>
{: .block-tip }
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>A tip can be used when you want to give advice related to a certain content.</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### WARNING</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a warning, and thus should</span>
<span class="gt">&gt; be used when you want to warn the user</span>
{: .block-warning }
</code></pre></div></div> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>This is a warning, and thus should be used when you want to warn the user</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### DANGER</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a danger zone, and thus should</span>
<span class="gt">&gt; be used carefully</span>
{: .block-danger }
</code></pre></div></div> <blockquote class="block-danger"> <h5 id="danger">DANGER</h5> <p>This is a danger zone, and thus should be used carefully</p> </blockquote>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="blockquotes"/><summary type="html"><![CDATA[an example of a blog post with custom blockquotes]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://jaeheon-lee486.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar"/><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p> <h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2> <p>To add a table of contents to a post as a sidebar, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p> <h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2> <p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p> <h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="toc"/><category term="sidebar"/><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">a post with audios</title><link href="https://jaeheon-lee486.github.io/blog/2023/audios/" rel="alternate" type="text/html" title="a post with audios"/><published>2023-04-25T10:25:00+00:00</published><updated>2023-04-25T10:25:00+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/audios</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/audios/"><![CDATA[<p>This is an example post with audios. It supports local audio files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/epicaly-short-113909.mp3" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="audios"/><summary type="html"><![CDATA[this is what included audios could look like]]></summary></entry><entry><title type="html">a post with videos</title><link href="https://jaeheon-lee486.github.io/blog/2023/videos/" rel="alternate" type="text/html" title="a post with videos"/><published>2023-04-24T21:01:00+00:00</published><updated>2023-04-24T21:01:00+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/videos</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/videos/"><![CDATA[<p>This is an example post with videos. It supports local video files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div> <p>It does also support embedding videos from different sources. Here are some examples:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&amp;title=0&amp;byline=0&amp;portrait=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[this is what included videos could look like]]></summary></entry><entry><title type="html">line_profiler</title><link href="https://jaeheon-lee486.github.io/blog/2023/line_profiler/" rel="alternate" type="text/html" title="line_profiler"/><published>2023-04-21T01:06:51+00:00</published><updated>2023-04-21T01:06:51+00:00</updated><id>https://jaeheon-lee486.github.io/blog/2023/line_profiler</id><content type="html" xml:base="https://jaeheon-lee486.github.io/blog/2023/line_profiler/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[kernprof -l -v my_script.py]]></summary></entry></feed>