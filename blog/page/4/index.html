<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>blog - page 4 | JaeHeon Lee</title> <meta name="author" content="JaeHeon Lee"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🧠</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaeheon-lee486.github.io/blog/page/4/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">JaeHeon&nbsp;</span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <div class="header-bar"> <h1>al-folio</h1> <h2>a simple whitespace theme for academics</h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>&bull;</p> <li> <i class="fas fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <br> <div class="container featured-posts"> <div class="row row-cols-2"> <div class="card-item col"> <a href="/blog/2021/distill/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a distill-style blog post</h3> <p class="card-text">an example of a distill-style blog post and main elements</p> <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp; <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> </p> </div> </div> </div> </div> </a> </div> <div class="card-item col"> <a href="/blog/2015/code/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a post with code</h3> <p class="card-text">an example of a blog post with some code</p> <p class="post-meta"> 4 min read &nbsp; &middot; &nbsp; <a href="/blog/2015"> <i class="fas fa-calendar fa-sm"></i> 2015 </a> </p> </div> </div> </div> </div> </a> </div> </div> </div> <hr> <ul class="post-list"> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Semi-Supervised-Classification-with-Graph-Convolutional-Networks" target="_blank">[Paper Review] Semi-Supervised Classification with Graph Convolutional Networks</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="semi-supervised-classification-with-graph-convolutional-networks">Semi-Supervised Classification with Graph Convolutional Networks</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/778aa56d-a3ed-4a9e-b01c-eace4e3f27fa/image.png" alt=""></p> <p>드디어 ASCO 학회 제출이 끝났다! 조금 여유가 생기기도 했고 graph 쪽을 전부터 공부해보고 싶었는데 마침 동료분이 graph 사용해서 whole slide image 분석하는 모델 시도해 보신다고 해서 겸사겸사 나도 시작했다. Graph 쪽 처음 본 논문은 사실 이 논문이 아니라 nature biomedical 논문인데 그건 다음 논문리뷰로 작성해보도록 하겠다. 이번 논문 GCN 은 spectral 관점에서 graph node 간의 관계를 고려해서, node 가 가지고 있는 feature 를 layer 에 통과시켜 각 노드가 어떤 class 에 해당하는지 분류하는 문제를 푸는 연구를 담고 있다. </p> <h2 id="introduction">Introduction</h2> <p>graph data 특성 상 각 node 마다 label 이 매우 적게 존재하기 때문에 graph node classification problem 은 보통 semi-supervised learning 으로 frame 된다. 기존 방식들은 label 이 존재하는 데이터에 대해 loss 를 따로 계산하고, label 이 존재하지 않는 데이터에 대해 graph Laplacian regularization term 을 이용해서 다음과 같이 계산해 왔었다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/1f1b1712-b36d-4244-a6dc-7c5cba2e5837/image.png" alt=""></p> <p>하지만 이 regularization term 은 &quot;노드 사이의 거리가 가까우면, feature 가 비슷할 것이다&quot; 라는, 다소 다른 추가적인 정보를 버릴 위험이 있는 term 이기 때문에, 저자는 이러한 term (explicit graph-based regularization) 사용을 지양하고자 하였다.</p> <p>이들의 contribution 은 다음으로 요약할 수 있다.</p> <ul> <li>introduce simple and well-behaved layer-wise propagation rule for NN moel which operate directly on graphs</li> <li><blockquote> <p>how it can be motivated from a first-order approximation of spectral graph convolutions</p> </blockquote> </li> <li>demonstrate speed and scalability in semi-supervised classification of nodes in a graph</li> </ul> <h2 id="background-notes">Background, notes</h2> <p>여기부터는 그 전 논문들에서 다루고 있는 spectral graph convolution 에 대한 이해가 필요하다. 따라서 논문의 내용을 설명하기 전 다음 블로그를 참고하여 정리한 노트를 첨부한다. 노트의 Chebyshev expansion 까지가 background 이고, reparameterization 및 rescaling 등의 trick 은 이번 GCN 논문에서 소개된다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/c70e205f-15d8-4ec7-a619-b833c12c0a2e/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/8fb2e393-3d14-4a6f-8003-8fd3b9c76f93/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/21671647-7277-49b4-9123-474f57a375bc/image.png" alt=""></p> <p>이 때, computationally expensive 하기 때문에 Chebyshev polynomial 을 사용한다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/144737da-cf27-4f65-883f-32aba869c9de/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/a4cb737c-5802-43b8-9ed1-40f85761e945/image.png" alt=""></p> <p>출처: <a href="https://theaisummer.com/graph-convolutional-networks/">https://theaisummer.com/graph-convolutional-networks/</a> </p> <h2 id="fast-approximate-convolutions-on-graphs">Fast Approximate Convolutions on Graphs</h2> <p>multi-layer Graph Convolutional Network (GCN) 은 다음과 같은 layer-wise propagation rule 로 정의된다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/a75ea228-1837-474c-b946-1197b23eca35/image.png" alt=""></p> <p>이 때 A는 self-connection 이 추가된 adjacency matrix 이고, D는 self-connection 이 추가된 A의 D이다. non linear function 은 ReLU function 을 사용했고, $H^{(l)}$ 은 N x D l번째 layer 을 뜻하며, 이에 상응하여 $H^{(0)}$ 은 데이터 그 자체 $x \in$ N 이다</p> <h3 id="spectral-graph-convolutions">Spectral Graph Convolutions</h3> <p>x 라는 graph signal 에 $g_{\theta}=diag(\theta)$ filter parameterized by $\theta \in \real^N$ in Fourier domain 를 붙인 형태로, spectral convolution 이 정의된다. ex) <img src="https://velog.velcdn.com/images/jaeheon-lee/post/8c692fb0-15cb-402b-bd13-dfa0df309f69/image.png" alt=""></p> <p>이 때, U 는 normalized graph Laplacian 의 eigenvector 이고, eigenvalue 는 graph Fourier transform of x 가 된다. 하지만 이런 방식을 사용하면, 1) L 에 대해 SVD 를 적용해야 하고 2) eigenvalue matrix U 를 multiplication 해야 함으로 computation cost 가 크다. 이러한 문제점을 극복하기 위해, 이전에 Chebyshev expansion 을 사용하여 series 형태로 <strong>각 locality 에 해당하는 filter 통과 feature</strong> $T_k(x)$ 의 합을 다음과 같이 표현했다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/f846ead3-3e4c-406c-afdc-3c3c2798fd32/image.png" alt=""></p> <p>$$$ Y = g_\theta(\tilde{L_h})X = [\tilde{X_0}, \tilde{X_1}, ... , \tilde{X_{k-1}}]\theta_v, \tilde{X_p}=T_p({\tilde{L_h}} )X $$$</p> <p>여기까지 사전지식에 해당한다. 각 locality k 에 대해 계산된 filter 를 통과한 feature 들이 concat 된 형태로, inception module 과 닮아있다.</p> <h3 id="layer-wise-linear-model">Layer-wise Linear Model</h3> <p>위에서 한 layer 를 표현했기 때문에, neural network model 도 convolution layer 형태를 띈 Equation (5) 를 여러개 stacking 함으로써 만들어 낼 수 있다. 이 때 한 layer 를 정의하기 위해, $\lambda_{max}$ 가 대략 2라고 가정하고 K=1 인 경우를 생각해보면, free parameter $\theta&#39;_0,\theta&#39;_1$ 와 함께 다음처럼 표현된다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/25a65794-7064-4f07-a295-5dfc95312a6d/image.png" alt=""></p> <p>이 때, filter parameter 는 whole graph 에서 share 가능한 형태이므로, successive application of filter 를 stack 해서 사용하다면, kth - order neighborhood of node 를 표현할 수 있게 되는 것이다. (위에서 k 는 locality 라고 표현했었음.)</p> <p>또한 실용적인 측면에서 저자는 새로운 renormalization trick 을 제안한다. 두개의 free parameter 값을, 두 parameter 간의 관계를 정의함으로써 하나로 변경하고 표현하면 다음과 같이 표현된다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b8910f58-56a7-49aa-852b-4a5c982ccc0e/image.png" alt=""></p> <p>이 때, 가운데 In+DAD 항은 가정된 대로 0-2 사이의 eigenvalue 값을 가지게 되는데, repeated application 을 거치면 numerical instability 와 exploding/vanishing gradient 를 초래할 수 있다. 이를 해결하기 위해, </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/9e47d29f-0bf8-415a-9f65-a87e86e85f90/image.png" alt=""></p> <p>다음과 같은 renormalization 과정을 거쳐, 다음과 같은 일반적인 형태를 얻도록 하였다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/aedb56a6-6598-43c6-8c47-97798c6df0d4/image.png" alt=""></p> <h2 id="semi-supervised-node-classification">Semi-Supervised Node Classification</h2> <p>위에서 언급한 구조를 정리하면 다음과 같다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/8a488d4f-9718-447a-8cd6-ec05816730e5/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/78de0f55-a7d2-48a1-a45f-35e821180b94/image.png" alt=""></p> <p>이 때 loss 는 <strong>labeled examples</strong> 에 대해서 cross-entropy 형태로 사용한다.</p> <h2 id="experiment--result">Experiment &amp; Result</h2> <p>3개의 citation network 에서의 document classification task 로 수행하였다. Dataset 은 다음과 같다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/6e873d9c-a1ff-4219-bbfa-7192e3263bc7/image.png" alt=""></p> <p>model 은 기본적으로 2-layer shallow network 를 사용하였고, supplementary result 에 layer 개수를 늘린 실험 결과를 나타내었다. 1000개의 labeled example 로 test 하였고, 여러 baseline 과 함께 성능을 기록하였다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/dc458c70-1885-4ca3-8ecf-7aaafd4fe701/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/5559e279-54e4-4923-ba7e-7425db250f3a/image.png" alt=""></p> <p>다른 propagation logic 을 사용해서 성능을 비교하였다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/cda4047f-8ec0-4391-b2d5-cfabc55d1327/image.png" alt=""></p> <h2 id="discussion">Discussion</h2> <p>다음과 같은 한계점을 언급하였다.</p> <ul> <li>Memory requirement grows linearly in the size of the dataset</li> <li>Directed edges and edge features are limited </li> <li>Limiting assumptions </li> <li>-&gt; self connection 과 edge 간의 importance 를 동일하게 취급함.</li> <li>-&gt; implicit하게 locality 를 가정함. (Kth layer 는 Kth order neighborhood이다.)</li> </ul> <p>재밌구만</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; February 22, 2023 &nbsp; &middot; &nbsp; velog.io </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Development-and-validation-of-a-deep-learning-algorithm-for-improving-Gleason-scoring-of-prostate-cancer" target="_blank">[Paper Review] Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="development-and-validation-of-a-deep-learning-algorithm-for-improving-gleason-scoring-of-prostate-cancer">Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/e2f33b55-4342-4db5-b05b-50672a220a15/image.png" alt=""></p> <p>최근 맡아 진행중인 프로젝트와 가려는 방향이 비슷한 논문이다. nature npj digital medicine 논문으로 2019년에 accept 되어 311번 인용되었다. (이 논문은 꼭 리뷰 해야겠다 다짐했는데 지금에서야 ... 하게 되었다.)</p> <h2 id="introduction">Introduction</h2> <p>Prostate adenocarcinoma (cancer) 는 남성에게 2번째로 많이 발병되는 암종이다. 이 환자의 중증도를 stratify 하는 방법으로, prostate resection 이나 prostate biopsy 의 H&amp;E image 를 보고 pathologist 가 morphologic feature 에 따라 gleason score 를 메겨 clinical treatment 를 정하거나 환자의 예후를 예측하는데 이용해 왔다. 하지만 이러한 방법은, pathologist 에 따라 subjective 한 scoring 이 잦고 (보고되어 왔고) 같은 환자에 대해 reproducibility 가 떨어진다는 문제가 있어왔다. 저자는 이러한 문제들을 해결하기 위해 deep learning 알고리즘을 이용한 더 세분화된 gleason score 를 propose 했고 이에 대해 validation 하는 연구를 진행했다.</p> <h2 id="results">Results</h2> <h3 id="overview-of-the-deep-learning-system-and-data-acquisition">Overview of the deep learning system and data acquisition</h3> <p>two-stage deep learning system 을 활용하였다. 첫번째는, regional Gleason pattern classification, 그 뒤로 K-nearest-neighbor-based whole-slide Gleason Grade group classification 을 수행했다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/3cbd4c16-27bf-4a65-9a25-38ed031ef3a6/image.png" alt=""></p> <p>첫번째 과정은, pathologist 의 pixel-level annotation 을 기반의 지도학습으로 학습되었고, 총 912 장의 슬라이드로부터 112,000,000 장의 patch 가 사용되었다. validation set 은 3 source 로부터 331 patient 의 331 slide 가 이용되었다. (TCGA, Tertiary teaching hospital, Medical laboratory)</p> <h3 id="comparision-of-dls-to-pathologists-on-whole-slide-gleason-scoring">Comparision of DLS to pathologists on whole-slide Gleason scoring</h3> <p>Validation dataset 에 대하여, 두 그룹의 리뷰 (예측) 을 비교하였다. </p> <ul> <li>29 pathologists in classifying each slide&#39;s Gleason Grade Group</li> <li>Deep learning system (DLS) performance <img src="https://velog.velcdn.com/images/jaeheon-lee/post/3b6070aa-c871-4c3e-87f6-240e8229932a/image.png" alt=""></li> </ul> <p>a 결과 DLS 0.7 acc &gt; pathologist 0.61 acc 였다. b 10명의 subgroup 을 뽑아, 각각 DLS 와 비교한 결과, 8명의 pathologist 가 DLS 보다 낮은 성능을 보였다.</p> <p>추가로, GG &gt;= 2, 3, 4 에 대해 decision threshold 를 관찰했다. DLS 는 AUC 값 0.95-0.96 을 보였고, 특히 Grade group &gt;= 4 에서 10명의 pathologist 중 9명의 pathologist 보다 좋은 성능을 보였다. </p> <h3 id="comparison-of-dls-to-pathologists-on-gleason-pattern-quantitation">Comparison of DLS to pathologists on GLeason pattern quantitation</h3> <p>ISUP와 CAP, WHO, recent publication 에서 권장되는 방법은 각 패턴이 슬라이드에서 차지하는 정확한 비율이 중요하다. 예를 들어, 5% 가 넘는지에 따른 추가 rule 이 존재한다. 이를 확인하기 위해, 미리 얻었던 pathologist의 pixel-level annotation 과 DLS score 를 비교하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/5702a284-e6f1-4c3a-a2f2-70f5b354526a/image.png" alt=""></p> <p>결과 GP5 의 비율에서는 근소한 차이를 보였으나, GP3, 4 에서는 significant 한 accuracy 차이를 보여주었다. 또한, 5% rule에 따라 grade group 이 바뀔 수 있는 일부 슬라이드만 subgroup 해서 같은 분석을 진행하였고, 이 validation set 에서도 acc 차이를 보였다.</p> <h3 id="insights-from-dls-region-level-classifications">Insights from DLS region-level classifications</h3> <p>region-level classification of DLS 를 평가하기 위해 3명의 pathologist의 79장 슬라이드 annotation 과 DLS prediction 결과를 다각도로 분석하였다. DLS는 97%, 3 pathologist 는 88% concur 하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/df925c81-2907-45a0-8b56-c72b0bd62264/image.png" alt=""></p> <p> b 에서 DLS 의 confidence score (각 패턴 별 probability) 를 as a function of inter-pathologist aggreement 에 따라 나타내었다. pathologist 가 Gleason pattern 3 에서 concordant, gleason pattern 3과 4 사이에서 disconcordant, concordant on GP4 했던 tissue region 에 대해서, DLS의 prediction score 결과는 smooth 하게 바뀌었다. 즉 confidence score 가, pathologist 의 scoring 과정 중 ambiguous 했던 부분까지 반영했다는 것을 암시한다. 이러한 trend 가 3-4 뿐만 아니라 4-5 pattern 에서도 관찰되었다. </p> <p>이를 fine-grained Gleason patterns (3.3이나 3.7과 같은) 로 분류하여 보여준 결과, spectrum 상에 well-to-poor differentiation 을 확인할 수 있었다.</p> <h3 id="measuring-effectiveness-of-gleason-scoring-in-risk-stratification-for-disease-progression">Measuring effectiveness of Gleason Scoring in risk stratification for disease progression</h3> <p>마지막으로 DLS 및 pathologist cohort 의 ability 를 확인하기 위해, 각 prediction 이 biochemical recurrence 또는 disease progression 이라는 이벤트에 대한 patient 의 위험도를, well-~ staratify 하는지 분석하였다.</p> <p>c-index 상으로 확인했을 때, pathologist-provided grade group 은 in average c-index 0.63 을 기록했고, DLS-predicted grade group 은 in average c-index 0.65 를 기록하였다. Kaplan-Meier and hazard ratio analyses using a binary GG&gt;=3 threshold 를 conduct 한 결과, 잘 나뉘는 것을 확인할 수 있었다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/6bef15b1-1b0b-42d9-8292-70656b09833b/image.png" alt=""></p> <p>추가 분석을 진행했고, 다음과 같은 결과를 관찰할 수 있었다.</p> <ul> <li>Cox model 을 써서 quantified Gleason patterns 의 prognostic ability 를 확인한 결과, DLS 0.697, 29 cohort 0.674 를 기록했다.</li> <li>Fine grained Gleason pattern 을 poc 하기 위해, GP3.5 를 추가하여 Cox model 로 확인한 결과, DLS 0.704, GP4.5 추가한 결과 0.702 로 향상.</li> </ul> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; January 25, 2023 &nbsp; &middot; &nbsp; velog.io </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-LassoNet-Neural-Networks-with-Feature-Sparsity" target="_blank">[Paper Review] LassoNet: Neural Networks with Feature Sparsity</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="lassonet-neural-networks-with-feature-sparsity">LassoNet: Neural Networks with Feature Sparsity</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d08f731a-31b6-465d-a0a9-4523c9723065/image.png" alt=""></p> <p>이번 논문은 2020년 스탠포드에서 쓴 XAI 관련 연구이다. 이 연구의 후속 연구인 FastCPH model 이 얼마전 1월에 나와서 읽는 중에 이 논문은 리뷰하면 재밌을 것 같았다. 지금까지 해석 가능한 AI 와 관련된 많은 연구가 이루어졌고, 하나의 접근 방식은, 네트워크가 중요한 feature 만 사용하도록 arrange 하는 것이다. 주로 Linear model 에서 Lasso (l1-penalty) regularization 은 가장 관련없는 feature 에 0 을 부여하는 방향으로 학습이 이루어지도록 하지만, linear model 에만 적용 가능했다. 이번 논문에서는 nonlinearity 를 머금고 있는 neural network 에 feature sparsity 를 수행하도록 강제하는 새로운 학습 방법 및 알고리즘을 제안한다. 해당 논문의 요약은 다음 영상에서 확인할 수 있다. <a href="https://www.youtube.com/watch?v=bbqpUfxA_OA">https://www.youtube.com/watch?v=bbqpUfxA_OA</a></p> <h2 id="introduction">Introduction</h2> <ul> <li>예측 문제에서 much of information in features is irrelevant</li> <li>high dimensional data - speech, object recognition, protein data</li> <li>benefits of feature selection include, 1) reduce experimental cost, 2) enhance interpretability 3) speed up 4) memory</li> <li>motivating question: Are there redundant or unnecessary features?</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/e67d00be-5da3-447f-8f40-21439af52fed/image.png" alt=""></p> <p>MICE protein dataset 에서 20%의 feature 만으로 70%의 signal 을 잡아냄. 절반 이하 35개의 feature 만으로 좋은 성능.</p> <h3 id="proposed-method">Proposed method</h3> <p>new approach LassoNet : extends Lasso regression and its feature sparsity - to FeedForwardNeuralNetworks. : input-to-output residual connection 을 활용하여, allow a feature to have non-zero weight in a hidden unit only if its linear connection is active : linear-nonlinear components are optimized jointly, allowing to capture arbitrary nonlinearity : outperforms state-of-the-art methods for feature selection and regression</p> <h2 id="our-proposal-lassonet">Our proposal: LassoNet</h2> <h3 id="background-and-notation">Background and notation</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/fde22ade-8b9d-4d1e-9925-5f3ea3250911/image.png" alt=""></p> <p>n: total number of training points d: data dimension $f_W$: fully connected feed-forward network with parameters W K: the size of the first hidden lyaer $W^{(1)}$: first hidden layer (dxK-dimensional) $\theta$: residual layer (d-dimensional) $S_{\lambda}(x)$: sign(x)max(|x|-$\lambda$, 0), soft thresholding operator</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/97ccf9e8-00a0-415d-8939-696d7b54ecf0/image.png" alt=""></p> <p>green: single residual connection, black: arbitrary feed-forward neural network. The residual layer and the first hidden layer are jointly passed through a hierarchical soft-thresholding optimizer</p> <p>1) penalty 는 기존 feature sparsity 로 이끄는 empirical risk minimization 을 따른다. combinatorial search 에서 continuous search by varying the level of the penalty. 2) proximal gradient algorithm 은 수학적으로 elegant way 로 적용되어, simple - efficient 적용이 가능하다. </p> <h3 id="formulation">Formulation</h3> <p>이 논문에서 가장 핵심이라 말할 수 있는 부분이 다음 식이다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/7dd28f62-7fc2-4f26-b6d2-af43c7d4f01c/image.png" alt=""></p> <p>특히 아래 constraint 부분이 포인트라고 할 수 있다. M 이라는 constant 에 따라, non-linearity involving feature j according to the relative effect importnace of $X_j$ 의 양이 조절된다. 즉, M=0 일 때 formulation 은 linear model 에서의 LASSO 와 같아진다. 반대로 M-&gt;inf 로 가면 standard feed-forward network with $l_1$-penalty on first layer 가 된다. </p> <p>이 formulation 은 몇가지 장점이 있는데, (의역 불가) linear component of the signal above the nonlinear one and 이를 feature sparsity로 이끈다. (이는 hierarchy priciple 이라는 통계학의 개념과 유사하다고 한다. 완전히 새로운 것은 아니라 함) 또한, linear and non-linear component 를 학습 과정 상 &quot;simultaneously&quot; 배우게 된다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/7baf7467-8019-4a4e-ab88-c83f435c4ce4/image.png" alt=""></p> <p>위 알고리즘은 LassoNet 의 training 과정이다. 먼저 모델 파라미터는 stochastic gradient descent 로 업데이트 된다. 이후 <strong>hierarchical proximal operator</strong> - optimization section 에 등장 - 가 input layer pair $(\theta, W^{(1)})$ 에 적용된다. elegant 하다는 표현이 참 적합하다 느껴졌다. 하지만 뒤에 무시무시한 증명 과정이 기다리고 있다.</p> <h3 id="hyper-parameter">Hyper-parameter</h3> <p>LassoNet 에는 두 가지 hyper-parameter 가 존재한다.</p> <ul> <li>the $l_1$-penalty coefficient $\lambda$ : controls complexity of the fitted model. </li> <li>the hierarchy coefficient $M$: controls the relative strength of the linear and nonlinear components</li> </ul> <p>특히 hierarchy coefficient 는 domain knowledge 없이 설정하는 것이 어렵다고 한다. 미리 정해둔 hyperparameter set 에서 parallel 하게 tuning 했다고 한다.</p> <h2 id="optimization">Optimization</h2> <h3 id="warm-starts-a-path-from-dense-to-sparse">Warm starts: a path from dense to sparse</h3> <p>feature selection 과정을 생각해보면 forward selection, reverse selection 이 있다. 저자는 전자와 후자를 각각 실험했고, 다음과 같은 결과를 얻었다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/5bb9ec03-1ea3-45a4-9123-19e0ef2cba1b/image.png" alt=""></p> <p>오른쪽 figure 를 보면, Lasso 와 LassoNet sparse to dense (red) 에 비해, LassoNet dense to sparse (green) 의 Test error 가 월등히 낮은 것을 확인할 수 있다. </p> <h3 id="hierarchical-proximal-optimization">Hierarchical proximal optimization</h3> <p>위 알고리즘에 등장했던 HIER-PROX() 라는 operator 에 대한 내용이다. 앞서도 간단히 설명했지만, 이는 numerically efficient &quot;algorithm&quot; 이다. (not DL model) 미리 그 존재와 유일성이 증명된 global minima 식을 통해 각 node 의 weight 들을 최적화 하는 과정이다. 필자는 다음과 같이 설명한다. Underlying its development is the derivation of equivalent optimality conditions that completely characterize the global solution of the no-convex minimization porblem defining the proximal operator. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/5e46cafb-b011-4c95-a910-1fb80d55a600/image.png" alt=""></p> <p>해당 식에서 주목할 점은, inner loop 에서 각 input feature 마다 따로 계산이 가능하다는 점이다. 또한 ranking 을 정해서 하나의 m 을 잡고 이를 이용해서 update 한다는 점이다. 이것이 가능한 이유는 다음 proposition 이 증명되었기 때문이다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/305c7bb1-4bcc-478e-8c01-0062b619c854/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/30f7f967-1669-45e0-9855-02dc3cf71832/image.png" alt=""></p> <p>증명과정을 간단히 요약하면, non-convex optimization problem - (KKT condition, strong duality) 을 두 가지 subproblem 으로 나눈다. 이 두 subproblem 은 각각 stochastic gradient descent 와 analytic한 관점에서 iterative 하게 문제를 해결한다. </p> <h2 id="experiments">Experiments</h2> <p>다음 데이터셋을 이용해 실험한다.</p> <ul> <li>ISOLET (617 dimension): speech data of people speaking the names of letter (alphabet)</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/cf3a5286-23fe-452c-a70f-9d81ab47cce2/image.png" alt=""></p> <p>LassoNet 으로 유의마한 feature extraction 을 수행한 뒤, decoder classification acc 와 XTree classification acc 를 관찰하였다. </p> <h2 id="application-to-unsupervised-feature-selection">Application to Unsupervised Feature Selection</h2> <p>MNIST 데이터셋을 이용한 feature extraction 성능을 확인하기 위해, decoder 로 이미지를 복원했다. 특히, GROUP-LASSO 알고리즘을 사용하여, 모든 같은 set 의 selected feature 가 all reconstructed input 에 동일하게 적용되도록 하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/58737a03-7afe-43ee-a985-fadbb2e46f6b/image.png" alt=""></p> <p>왼쪽은 test images, 오른쪽은 LassoNet 으로 복원한 결과이다.</p> <h2 id="application-to-matrix-completion">Application to Matrix Completion</h2> <p>몇몇 biomedical data 등의 데이터는 종종 measurement 자체의 cost 가 높거나 데이터 수집 날짜 등의 다양한 요인에 의해 데이터 소실 문제에 부딪힌다. 즉 missing row 가 많은 편이다. 이를 해결하기 위해 matrix completion problem 이 연구되어 왔는데, 대표적으로 soft-impute algorithm 은 underlying data 가 low-rank 가정을 요한다. (singular value 이용) </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/a09a97c3-7ba5-4747-978f-2bb5e898be09/image.png" alt=""></p> <p>LassoNet 의 결과와 함께 기존 방식의 한계점을 지적한다.</p> <ol> <li>low rank assumption 이 항상 모든 데이터에 적합하지 않다. </li> <li>model 의 linear assumption 이 들어맞지 않을 때 성능이 크게 하락한다.</li> </ol> <p>LassoNet 은 iterative 하게 적절한 feature 를 선택하며 feature 개수까지 tuning 가능하고, hyperparameter 를 통해 데이터의 linearity 뿐만 아니라 non-linearity 또한 포함할 수 있기에 성능이 더 잘 나올 수 있었다고 설명한다.</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; January 25, 2023 &nbsp; &middot; &nbsp; velog.io </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p></li> <li><h3> <a class="post-title" href="/blog/2022/giscus-comments/">a post with giscus comments</a> </h3> <p>an example of a blog post with giscus comments</p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; December 10, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> &nbsp; &middot; &nbsp; <a href="/blog/tag/comments"> <i class="fas fa-hashtag fa-sm"></i> comments</a> &nbsp; &nbsp; &middot; &nbsp; <a href="/blog/category/sample-posts"> <i class="fas fa-tag fa-sm"></i> sample-posts</a> &nbsp; <a href="/blog/category/external-services"> <i class="fas fa-tag fa-sm"></i> external-services</a> &nbsp; </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-VICRegL-Self-Supervised-Learning-of-Local-Visual-Features" target="_blank">[Paper Review] VICRegL: Self-Supervised Learning of Local Visual Features</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="vicregl-self-supervised-learning-of-local-visual-features">VICRegL: Self-Supervised Learning of Local Visual Features</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b7c932cc-cc96-4b1d-a3a9-d890e8ba3d3c/image.png" alt=""></p> <p>2021년 facebook AI (FAIR) 에서 연구했던 VICReg 의 후속 연구로 VICRegL 가 얼마전 Neurips2022 에 accept 되었다. VICReg 는 contrastive learning loss 와 non-contrastive loss 을 동시에 계산해 weighted sum 하여 모델의 collapse 를 방지하고자 하였다. 본 연구에서는 VICReg의 loss 를 사용하되, 중간 단계에서 local property 를 활용하고자 feature map 을 활용해 local criterion 을 추가하여, local view 가 중요한 segmentation task 에서 state-of-the-art 성능을 기록하고, coefficient 에 따른 trade-off 를 관찰하였다. </p> <h2 id="introduction">Introduction</h2> <p>최근 self-supervised learning 방식은 image augmentation 을 통해 생성된 different view 의 image 를, joint embedding architecture 와 loss function 로 global feature 를 학습한다. semantic segmentation 과 같은, spatial information 이 중요한 역할을 하는 task 에서는, local image structure 도 focus 한다. </p> <p>small parts of image 를 묘사하는 local feature 를 배우기 위한 방법으로는 다음과 같이 세 개로 나눌 수 있다. </p> <ol> <li>pixel level, which forces consistency between pixels at similar location </li> <li>feature map level, which forces consistency between groups of pixels</li> <li>image region level, which forces consistency between large regions that overlap in different views</li> </ol> <p>핵심은, forces consistency, 비슷하게 생각할 것은 비슷하게 처리하도록 명령하고, 다르게 생각할 것은 다르게 처리하도록 명령하는 것이다. VICRegL 는 feature map level 에서, pixel space 상 가까이 있는 대상들뿐만 아니라, 멀리 떨어져 있는 object 간 고려할 수 있는 아키텍쳐를 제안한다. </p> <h2 id="method">Method</h2> <h3 id="background">Background</h3> <p>VICReg 는 variance term, invariance term, covariance term 을 활용한 self-supervised learning 이다. variance term 은 embedding vector 간의 variance 를 특정 standard deviation 아래로 떨어지지 않도록 hinge loss 를 사용한다. invariance term 은 l2 loss 를 이용해 Siamese architecture 의 두 branch 로부터 나온 feature 간의 거리를 계산한다. covariance term 은 embedding vectoro 의 covariance matrix 의 off-diagnoal term 을 0으로 보냄으로써, different dimension 을 decorrelate 하는 역할을 수행한다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/3763462b-f912-473e-9df1-9d30becaed92/image.png" alt=""></p> <h3 id="vicregl-feature-vectors-matching">VICRegL: feature vectors matching</h3> <p>위 그려진 VICReg 의 구조를 보면, y 가 그대로 global expander 를 통과해 특정 dimension (D) 로 mapping 된다. VICRegL 은 VICReg 의 global expander와 loss 를 그대로 사용하되, local feature 를 학습하기 위한 새로운 경로를 도입한다. seed image x 가 convolutional encoder 를 통과한 feature map (C x H x W) 을 y, x&#39;의 feature map (C x H x W) 의 feature map 을 y&#39; 이라 하자. 이 때, $y_{i,j}$ 는 C dimension 을 가진, 즉 H x W grid 상 한 점이라고 하자. 이 논문의 핵심은 이 $y_{i,j}$ 와 $y&#39;<em>{i,j}$ 를 적절히 match 시켜 VICreg 의 criterion 을 적용하는 것이다. 그 전에, y 와 y&#39; 을 local projector 를 통해 (D x H x W) dimension 을 가진 z 와 z&#39; 를 만들고, y 와 y&#39; 의 정보 &amp; z 와 z&#39;의 정보로 어떤 $z</em>{i,j}$ 와 $z&#39;_{i,j}$ 을 매치해서 계산할지 결정 짓는 것이다. 앞서 언급했듯이 matching 과정은, feature map 상 실제 거리를 고려한 location-based matching 과, 멀리 떨어져 있어도 같은 object 인 small part 도 고려하기 위한 feature-based matching 로 구성된다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b7c932cc-cc96-4b1d-a3a9-d890e8ba3d3c/image.png" alt=""></p> <h3 id="location-based-matching">Location-based matching</h3> <p>image I 에 대해 두 view x, x&#39; 의 similar location 으로부터 나온 feature 를 match 시켜 VICReg criterion 을 적용한다. 먼저 absolute position in I 에 따라, each feature vector $z_p$ at position p 는, its spatial nearest neighbor 과 match 된다 (1:1 match). 그리고 계산된 H x W pairs 중 only top-$\gamma$ 개의 pairs 만 keep 된다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/66874db2-982a-4ddf-9790-a05744088f09/image.png" alt=""></p> <p>이때, P 는 H x W 상의 점 하나를 의미하고, $z&#39;_{NN(p)}$ 는 p에 따라 결정되는 spatially closest coordinate p&#39; 를 의미한다. (위 식에는 top-$\gamma$에 대한 정보는 생략되어 있다.)</p> <h3 id="feature-based-matching">Feature-based matching</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/01bdc36a-10a6-4da9-bfd2-2a4dc15c2bf8/image.png" alt=""></p> <p>Feature-based matching 을 위해, embedding space 에서의 l2-distance 정보를 이용한다. feature vector $z_p$에 대해, feature map z&#39; 에서, l2 distance 가 가장 작은 feature vector NN&#39;($z_p$) 와 matching 하여 VICReg criterion 을 적용한다. 이를 통해, same location 에서 pooled 된 feature 가 아니더라도, long-range interaction 을 capture 할 수 있다. </p> <p>General idea of top-$\gamma$ filtering 은 1) location-based filtering 에서 너무 멀리 떨어져 있는 feature vector 간의 mismatch 막기 위함이 있고, 무엇보다도 2) training 의 초기 단계에서, feature-based matching 과정에서 다른 texture 나 다른 object 간의 mismatch 를 막기 위함이다. (나중에 gamma 를 바꿔가며 실험함.) 또한, location-based matching 의 경우, two view 가 겹치지 않을 수 있는데, 이는 확률이 낮고 관련 실험도 진행했을 때, 성능에 큰 영향을 주지 않았음도 언급하였다. </p> <h3 id="final-loss-function">Final loss function</h3> <p>최종 loss term 은 location-based &amp; feature-based loss function 을 combination 한 것과, global view 에 VICReg criterion 을 적용한 loss function 을 한 번 더 alpha 라는 coefficient 로 combination 한 형태를 띈다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/32efe6c5-d234-4f50-8ad9-46f4d289f389/image.png" alt=""></p> <h3 id="vicregl-with-the-convnext-backbone">VICRegL with the ConvNeXt backbone</h3> <p>이번 논문에서 ResNet-50 backbone 을 사용한 experimental result 도 보고 했지만, 다른 backbone 을 활용해서도 실험했고 성능이 향상되었음을 확인하였다. 2022년 제안된 ConvNeXt architecture 를 활용하여 state-of-the-art 성능을 확인하였고, 이는 self-supervised learning 에서 ConvNeXt 를 사용한 첫 시도라고 언급하고 있다.</p> <p>또한 Swav 논문에서 제안되어 사용되고 있는 multi-crop strategy 도 사용하였다. 간단히 언급하면, 기존 contrastive loss 는 두 개의 different view 들로부터 계산하지만, multi-crop strategy 는 N 개의 different view 들로부터 loss 를 계산한다. 구체적으로, 2개의 large crop 과 N-2 개의 small crop 을 사용하고, large image 만을 pivot 삼아 large-small (또는 large-large) view 간의 비교를 통해 computational efficient 와 performance importance 두 측면의 trade-off 를 최소화 하는 방식이다. 말로만 하면 이해가 안될테니 식으로 보면 다음과 같다. ($\gamma_1$=20, $\gamma_2$=4) </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/fba1da46-3b35-4fdd-8d25-76e7798b107c/image.png" alt=""></p> <p>m 은 large crop image, n 은 small crop image 를 의미한다. 원래 N 개의 different view 를 사용하면, $N^2$ 번의 비교를 거쳐야 하지만, 다음과 같은 방식을 사용하면 $2(N-1)$ 번의 computation 만 거치면 된다. location-based matching loss 이외에도 feature-based matching loss 와 기존 global loss 에도 비슷하게 적용된다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d1b86e03-26ef-4819-bcb3-09456860aa11/image.png" alt=""></p> <h3 id="implementation-details">Implementation details</h3> <ul> <li>best ResNet-50, ConvNeXts</li> <li>pretrained on the 100-class unlabeled ImageNet dataset</li> <li>most hyperparameter unchanged, Bardes et al., 2022</li> <li>VICReg criterion coefficient ratio 25:25:1</li> <li>global expander is 3-layers fully-connected network (2048-8192-8192-8192)</li> <li>local projector, due to memory limitation, small (2048-512-512-512)</li> <li>GPU: Nvidia Tesla V100-32Gb GPU, R50:32(2048), Cs:8(384), Cb:16(572)</li> </ul> <h2 id="experimental-results">Experimental Results</h2> <p>다양한 조건을 바꿔가며 실험하였다. main observation 으로, VICRegL 은 strongly improves on segmentation results over VICReg 와 동시에, preserving classification performance 였다. </p> <h3 id="comparision-with-prior-work">Comparision with prior work</h3> <p><strong>ResNet-50 Backbone</strong></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d421687b-0607-49c5-ac28-af9220b09ad1/image.png" alt=""></p> <p>made improvement of VICRegL over VICReg on linear segmentation, cityscape 는 대부분 성능 안좋음, global feature 와 local feature 를 동시에 사용한 것에 기인한 robustness.</p> <p><strong>ConvNeXt Backbone</strong></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d2db5f2f-611a-4b17-b871-fe4f9f72a982/image.png" alt=""></p> <p>linear segmentation task 에서, CNX-XL 을 사용했을 때 state-of-the-art 달성 alpha 값에 따른 classification / segmentation performance trade-off</p> <h3 id="ablations">Ablations</h3> <p>CNX-S ImageNet over 100 epoch 결과로 통일. linear classification 및 linear frozen segmentation mIOU 각각 ImageNet 과 Pascal VOC 에서 측정</p> <p><strong>Trade-off between the local and global criterion</strong> <img src="https://velog.velcdn.com/images/jaeheon-lee/post/35202acd-4725-494a-9720-f5347f7af18c/image.png" alt=""></p> <p>alpha&lt;1 을 적용했을 때, segmentation performance is greatly increased existence of a sweet spot, where the model performs both tasks</p> <p><strong>Study of the importance between feature-based and location-based local criteria</strong> <img src="https://velog.velcdn.com/images/jaeheon-lee/post/294ce5b2-5a5a-4a4e-8067-875b600d0ef7/image.png" alt=""></p> <p>두 component 가 함께 할 때 classification performance 방어와 segmentation performance improvement 면에서 의미가 있음.</p> <p><strong>Study of the number of matches</strong> multicrop 을 사용하면, large view와 small view 가 만들어지고 그에 따른 feature map 크기도 (7x7), (3x3) 으로 달라진다. 이에 따라 top-$\gamma$ 의 gamma1, gamma2 를 나누었고 이 값들을 달리하여 실험을 진행하였음.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/1ff6d2ee-6d00-42a3-8c72-4b5a291c5d5b/image.png" alt=""></p> <p>최소한으로 줄이거나 최대로 늘렸을 때도 성능이 괜찮았지만, between 값을 했을 때 가장 좋음.</p> <p><strong>Study of VICReg components for the local criterion, Multi-crop strategy</strong> <img src="https://velog.velcdn.com/images/jaeheon-lee/post/b7cd6144-20d8-4c99-a6f8-9b251d633512/image.png" alt=""></p> <p><strong>SimCLR + local feature network</strong> local feature network 에서 사용하던 VICReg criterion 대신 SimCLR loss 를 적용함. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/24462f4c-27ba-49d9-834e-95e571586c5e/image.png" alt=""></p> <p>SimCLR 보다 SimCLR-L 에서 additional benefit 이 확인되었고, VICReg와 VICRegL 사이의 additional benefit 의 폭이, SimCLR의 그것보다 더 컸음.</p> <h3 id="visualization">Visualization</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/cb95589a-182f-4432-8d4a-ef9781f3abbc/image.png" alt=""></p> <p>왼쪽 column 의 빨강파랑은 feature-based matching, 오른쪽 colum 의 빨강 파랑은 location-based matching 을 의미한다. 실험 상 20개의 best match 를 골랐지만 여기는 visualziation 을 위해 몇개를 생략하였다. 또한, 노란 선은 matching 된 pair 를 의미하고, grid 로 나누었지만 실제 receptive field 는 훨씬 크다. </p> <p>비오는 날엔 카페에서 논문 리뷰... 집중 잘된다 히히</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; October 9, 2022 &nbsp; &middot; &nbsp; velog.io </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p></li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/3/" tabindex="-1" aria-disabled="3">Newer</a> </li><li class="page-item "><a class="page-link" href="/blog/page/3/index.html" title="blog - page 3">3</a></li> <li class="page-item active"><a class="page-link" href="/blog/page/4/index.html" title="blog - page 4">4</a></li> <li class="page-item "><a class="page-link" href="/blog/page/5/index.html" title="blog - page 5">5</a></li> <li class="page-item "><a class="page-link" href="/blog/page/6/index.html" title="blog - page 6">6</a></li> <li class="page-item "><a class="page-link" href="/blog/page/7/index.html" title="blog - page 7">7</a></li> <li class="page-item "> <a class="page-link" href="/blog/page/5/">Older</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> &copy; Copyright 2023 JaeHeon Lee. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>