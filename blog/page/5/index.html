<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>blog - page 5 | JaeHeon Lee</title> <meta name="author" content="JaeHeon Lee"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🧠</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaeheon-lee486.github.io/blog/page/5/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">JaeHeon&nbsp;</span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <div class="header-bar"> <h1>al-folio</h1> <h2>a simple whitespace theme for academics</h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>&bull;</p> <li> <i class="fas fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <br> <div class="container featured-posts"> <div class="row row-cols-2"> <div class="card-item col"> <a href="/blog/2021/distill/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a distill-style blog post</h3> <p class="card-text">an example of a distill-style blog post and main elements</p> <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp; <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> </p> </div> </div> </div> </div> </a> </div> <div class="card-item col"> <a href="/blog/2015/code/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a post with code</h3> <p class="card-text">an example of a blog post with some code</p> <p class="post-meta"> 4 min read &nbsp; &middot; &nbsp; <a href="/blog/2015"> <i class="fas fa-calendar fa-sm"></i> 2015 </a> </p> </div> </div> </div> </div> </a> </div> </div> </div> <hr> <ul class="post-list"> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Handcrafted-Histological-Transformer-H2T-Unsupervised-Representation-of-Whole-Slide-Images" target="_blank">[Paper Review] Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="handcrafted-histological-transformer-h2t-unsupervised-representation-of-whole-slide-images">Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/8a714f41-32f3-48db-b220-8200058b12c9/image.png" alt=""></p> <h2 id="introduction">Introduction</h2> <p>HoverNet의 저자인 Quoc Dang Vu가 작성한 논문으로, SwAV 이라는 self-supervised learning 방식을 사용하여 WSI 의 각 patch 의 prototype vector 를 생성하고, 이 prototype vector 로 이루어진 representation space 에 각 WSI 의 patch 를 embedding 하여 feature 를 생성하였다. WSI 내 patch 간의 위치 정보도 활용할 수 있도록 pattern assignment map 을 활용하여 patch 정보를 WSI 단위로 aggregate 하였고, 이를 downstream analysis 에 활용하였다. patch 정보를 WSI 단위 정보로 aggregate 하는 과정에서 transformer 의 attention mechanism 과 유사한 방법이 사용되어, handcrafted histological transformer (H2T) 라 주장한다. 또한 interpretability 와 improvement in predictive power 사이의 trade-off 를 적절히 극복한 방법이라 주장한다.</p> <h2 id="method">Method</h2> <h3 id="handcrafted-histological-transformer-h2t">Handcrafted histological transformer (H2T)</h3> <p>Transformer 와 그 동작 원리를 밝힌 연구에 영감을 받아, H2T 를 설계하였다. H2T 는 two stage 로 이루어져 있다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/f485564d-cbde-41be-b3de-780f8dbd2e73/image.png" alt=""></p> <p>a. Construction of the prototypical patterns b. Projection against these patterns</p> <p>first stage 에서 여러 source 로부터 얻은 충분한 양의 reference WSI 를 patch 화 하여 prototypical pattern 을 추출한다. 특히 전에 리뷰했던 SwAV 라는 Self-supervised learning 방식, scalable online clustering, 을 활용하여 prototypical pattern (centroids) 를 계산한다. second stage 에서 새로운 WSI 를 patch 화 하여 prototypical pattern 에 대해 projection 하고, constituent instance 와 instance 가 assign된 pattern 간의 relationship 을 기반으로 한, WSI 단위의 summarized information 을 생성한다. </p> <h3 id="multi-head-self-attention">Multi head self-attention</h3> <p>transformer 의 multi head self-attention 에 대해 설명하고, 저자가 제안한 메커니즘과 synonuymous 임을 주장한다. 이번 섹션의 경우, 이해가 잘 되지는 않았다.. 다음은 K Q V relationship 을 나타내는 formulation 이다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/728dfc00-0043-44fc-8b0a-6444080bb708/image.png" alt=""></p> <p>Hopfield is all you need 논문을 참조했을 때, MHA와 Hopfield neural network 는 큰 관련이 있고, K 와 V 에 대해 same input Y 와 Q 를 rename 한 R 을 적용하면 다음과 같은 식을 얻을 수 있고, 이는 몇가지 흥미로운 properties 를 보인다 주장한다. (음.. 1x1 convolution 으로 channel 수를 늘리고 이를 Q, K, V 로 활용하는 것이 아니었나? Q와 K의 attention를 V에 곱해주는게.. 아 Y(instance)랑 R(prototypical vector)와의 유사도를 보는건가? 흠..)</p> <p> <img src="https://velog.velcdn.com/images/jaeheon-lee/post/6900d663-7861-4677-988e-774392178e8d/image.png" alt=""></p> <p>First, 위 식은 (R을 reference 삼아) inputs R과 Y 사이의 association 을 찾는 것과 동치이다. Second, scaling factor beta는 모델의 memorization 과 association capacity 를 결정 짓기에 중요한 요소이다. Finally, R이 trainable 이라 했을 때, traing set으로 부터 prototypical patterns P 의 집합을 배우는 아키텍쳐를 효과적으로 얻을 수 있다. (...) </p> <h3 id="positional-encoding">Positional encoding</h3> <p>위에서 언급한 식을 보면, instance 간의 관계는 permutation invariant 하다. 하지만 instance 의 ordering 즉 position 은 중요한 정보이기에, MHA 에서 푸리에 인코딩이나 sine-encoding 방식을 사용하여 positional encoding을 해준다. (이후 sine, cosine 을 이용한 positional encoding 설명)</p> <h3 id="handcrafted-prototypical-patterns">Handcrafted prototypical patterns</h3> <p>위 handcrafted histological transformer (H2T) 의 second stage 를 다시 두 가지로 나누면 다음과 같다.</p> <p>1) WSI 를 patch 호 하여 prototypical pattern 에 projection 한다. 2) constituent instance 와 instance 가 assign 된 pattern 간의 relationship 을 기반으로 한 WSI 단위의 summarized info 를 생성한다.</p> <h4 id="1-representation-from-histological-patterns">1) representation from histological patterns</h4> <p>Prototypical patterns 를 SwAV 방식으로 뽑아내고, WSI 를 어떻게 projection 하느냐에 대한 설명이 담겨있다. (처음에 이해하느라 힘들었다) <img src="https://velog.velcdn.com/images/jaeheon-lee/post/1257545a-cbe3-4d98-86d5-12d163aba67d/image.png" alt=""></p> <p>WSI 를 patch 화 하여 (적절한 backbone 을 거친) feature vector $\psi$ 와 모든 prototypical vector 간의 distance 를 계산하여, minimum distance 를 가진 pattern 에 assign 된다. 우선 $\Phi_i$ 는 각 pattern 에 assign 된 instance 의 개수를 의미한다. $f(p_i, \psi_i)$ 는 p와 psi 사이의 similarity 를 measure 해주는 attribution function 이다. 따라서 $H_i$ 는 i 번째 pattern 에 속한 vector 들이 얼마나 centroid 와 가까운지를 보여주는 단합력을 기반으로 instance 끼리의 정보를 합한 pattern 의 weight, importance 라 생각할 수 있다. 이를 각 pattern 별로 concat 하여 WSI 단위의 H 를 생성하는 것이다. </p> <p>이 때, $f(p_i, \psi_i)$, attribution function 을 다음과 같은 방식으로 다양하게 구성하여 실험을 진행하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/06de8b48-6915-4b5b-ac5d-aae1a32d4ad7/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/de3d134e-9668-442c-af0a-b36b0e5a5dff/image.png" alt=""></p> <h4 id="2-representation-from-co-localization-of-patterns">2) representation from co-localization of patterns</h4> <p>이제 patch instance 간의 spatial relation 을 기반으로 또 다른 WSI 단위의 정보를 뽑아낸다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/7f2b6fb5-3cd1-4576-a178-03ef58b242b8/image.png" alt=""></p> <p>pattern co-localization matrix (PCM) 으로 정의된 다음 matrix 는 $\gamma$ 값 (radius) 에 따라 각 pattern 끼리 얼마나 가까이에 분포해 있는지에 대한 정보를 담는다. 다음 사진을 보면 더 쉽게 이해할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/1231c8ed-976f-43d5-a6cc-d5da9c2e7eb0/image.png" alt=""></p> <p>8개의 immediate neighbors 을 기준으로 어떤 pattern 이 몇개가 인접해 있는지를 PCM 으로 나타낸 것이다. 저자는 이 방법을 사용했을 때의 문제점도 지적하고 있다. gamma 값을 어느 수준까지만 높일 수 있다는 unscalable 한 특성을 지적하며, CNN 을 사용할 것도 제안한다. 각 patch instance 의 prototypical pattern 을 assign 했고 patch 의 위치도 알고 있으므로, 그 spatial relation 을 2D CNN 으로 보고 하나의 map, pattern assignment map (PAM) 을 얻을 수 있다고 주장한다. 또한, PAM 은 raw pixel intensity 를 가지지 않고 discrete value 를 가지기 때문에, PAM 을 directly learn 하도록 설계하는 것보다 CNN 을 학습함으로써 Deep PAM feature 를 얻고, 이를 WSI 별로 C 라는 feature 로 추출할 것을 제안하고 있다. </p> <h2 id="experimental-results">Experimental Results</h2> <h3 id="datasets">Datasets</h3> <p>TCGA 와 CPTAC 의 2개의 암종 (LUAD, LUSC, Normal) 의 FFPE, Frozen slide 를 사용하여, 총 1245명의 환자자로부터 5306 장의 WSI 를 사용하였다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/dd3309c0-45d0-402b-baa3-776c557de511/image.png" alt=""></p> <h3 id="evaluation">Evaluation</h3> <p>여러 attribution function 을 사용해 WSI 로부터 추출한 H2T feature 는 linear probing 방식으로 downstream task 에 적용되어 성능을 측정하였다. CPTAC 와 TCGA 둘 중 하나를 training set 으로 사용하고, 나머지를 validation set 으로 활용하였다. 구체적으로 LUAD, LUSC, normal 사이를 classify 하는 task 를 5-fold crss validation 하여 수치를 기록하였다. </p> <h3 id="implementation-details">Implementation details</h3> <p>size 512x512 with 256x256 overlapping 방식으로 patch를 추출하였고, pretrained ResNet50 으로 기본 representation 을 뽑아내었다. 40x와 20x magnification 을 가진 WSI 를 사용하였고, SwAV 으로 하여금 16 x 2048 vector (16개의 cluster, 2048-dimensional) 를 학습하도록 설계하였다. </p> <h3 id="prototypical-patterns">Prototypical patterns</h3> <p>further assessment 이전에 SwAV prototypical pattern 의 sanity check 를 수행하였다. 4개의 set (normal, LUAD, LUSC, all) 의 TCGA WSI 와 SwAV-ResNet50 을 이용하여 16 prototypical pattern 을 뽑아내었다. 이후 pattern assignment 를 수행하고, pathologist annotation 과 비교하였다. train 은 TCGA 로 했지만 PAM 은 ACDC dataset 을 이용하여 map 을 그려보았다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/6e82d7ab-f6f8-4c9e-a9ae-2fab495fce76/image.png" alt=""></p> <p>6번과 16번 pattern 이 normal 과 관련을 보였고, 1번, 13번 은 LUAD tumor, 15번은 LUSC tumor pattern 과 연관성을 보였다. LUAD, LUSC 와 pathologist 의 annotation 과 각각 0.6879, 0.8407 의 Pearson Correlation Coefficient 를 기록하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b259692c-56c0-47e5-9321-ea05d2b2c485/image.png" alt=""></p> <p>같은 prototypical pattern 으로 reference cohort TCGA, unseen cohort CPTAC setting 에서 3 dataset LUAD, LUSC, normal 을 visualization 하였다. 결과 tumorous, stromal region 간의 pattern consistency 를 확인할 수 있었다. </p> <h3 id="comparative-evaluation">Comparative evaluation</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/00f73a60-0cbf-4fb5-8cce-6f69de5ac62f/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/4547a013-8cce-451d-8d1f-a18b8f24ec69/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b78c3512-b5f2-4bc2-9d17-cda1e1d08f18/image.png" alt=""></p> <p>normal vs tumor, LUAD vs LUSC, Normal vs LUAD vs LUSC WSI classification task 결과 5 fold AUROC (or mAP) 을 나타낸 결과이다. </p> <h3 id="abltaion-study">Abltaion study</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/58514402-6f32-4e1f-8d60-2d63b2439fda/image.png" alt=""></p> <p>CNN 을 활용하기 전 C 를 활용하는 방법에 따른 성능 수치이다. Normal vs Tumor classification task 이다. C는 co-localization matrix of patterns within PAM, C-raw 는 PAM 을 CNN 으로 학습, C-one-hot 은 pattern 을 one-hot encoding 한 후 CNN 에 넣어 학습한 결과 이다. </p> <h3 id="discovery-experiments">Discovery experiments</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/145371d7-9014-4182-8c58-5a713157620e/image.png" alt=""></p> <p>discovering anomalous WSI task 를 잘 수행하는 가도 추가 downstream task 로 진행하였다. 16 x 2048 feature, 즉 high-dimensional vector 를 visualization 하기 위해 UMAP 로 2D plane에 각 WSI 를 나타내었다. 결과 LUAD, LUSC 는 around 0.8 (orange) 에 머물렀지만, normal WSI 는 distinctly high anomaly score 를 가졌다. 또한 unsupervised clustering of WSI 측면에서도 LUAD, LUSC, normal 간 separation 을 관측할 수 있었다. </p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; September 25, 2022 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Book-Review-How-AI-and-the-Brain-work" target="_blank">[도서 리뷰] 인공지능과 뇌는 어떻게 생각하는가</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="인공지능과-뇌는-어떻게-생각하는가">인공지능과 뇌는 어떻게 생각하는가</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b9db6c65-704a-460a-bbf1-b0839a5eb35e/image.png" alt=""></p> <h2 id="--">--</h2> <p>논문 리뷰만 올리던 이 공간에도 드디어 다른 카테고리의 글을 쓰게 되었다. ㅎㅎ TensorflowKR (페이스북 그룹) 에서 작년 1년간 연구를 지도해주셨던 이상완 교수님 (갓) 이 책을 내셨다는 소식을 접했고, 마침 활동하기 시작한 brain and cognitive science community (BCSC) 에서 책을 지원해준다는 기쁜 소식을 듣고 바로 신청하였다. 리뷰를 시작하기 전, 필자는 AI와 computational neuroscience에 관심이 많고, 이미 책의 내용과 관련된 전공 수업과 프로젝트를 거친 학생임을 밝힌다.</p> <h2 id="---1">--</h2> <p>프롤로그와 7장에 거친 본문, 에필로그, 참고문헌으로 내용이 구성되어 있고, 난이도는 AI를 조금이라도 접해본 사람이면 쉽게 읽을 것으로 예상된다. 종이접기 로부터 시작되는 비유들을 보면서 교수님이 이 내용들을 <strong>쉽게</strong> 전달하고자 하는 강한 의지 (?) 를 느낄 수 있었고, 비유들이 모두 실제 <strong>개념을 왜곡 없이 설명</strong> 하고 있다는 느낌을 받았다. (실제로 이상완 교수님의 수업을 한 학기 들었었는데 수업 도중 이해도를 매번 설문조사로 확인하고 수업 후 질문이 몇개든 점심시간이 초과되든 상관없이 모두 들어주셨다.) </p> <p>아마 처음 접해본 신경과학 학도에겐 굉장히 어렵지 않았을까 .. 하는 생각이 든다. 본문의 6장을 제외하고는 모두 AI theory 에 관련된 내용이 주를 이룬다. 1장과 2장에서는 XOR problem 을 비롯한 shallow network 와 estimator, regularization, overfitting 에 대해 소개하고 있다. 종이접기라는 비유를 통해 network의 layer 부터 쭉쭉 설명해주었고, SVM, kernel-SVM, dual problem 에 대해 간단히 소개하고 있으며, 구조의 단순함과 bias-variance tradeoff 를 언급하였다. </p> <p>3장에선 본격적으로 deep learning model 에 대해 설명한다. 인공지능 모델 개념의 추상화 과정에서 필연적으로 발생하는 specificity-invariance 딜레마에 대해 소개하고, 이 두마리 토끼를 안정적으로 block 화 하여 깊게 쌓아올리는 CNN 방식에 대해 소개하고 있다. 또한 visual system 연구가 AI architecture 에 영향을 준 연구와 AI 가 visual / auditory cortex 해석에 영향을 준 연구 사례도 소개되고 있다. (흥미로움)</p> <p>4장에선 책 표지의 <strong>&quot;지극히 주관적인 그래서 더욱 객관적인&quot;</strong> 문구가 등장한다. 개인적으로 이 책에서 가장 어렵고 재밌는 부분이라 생각한다. 바로 generative model (개념의 구체화) 인데, hopfield model 과 Boltzmann machine, restricted boltzmann machine, autoencoder, VAE, GAN 에 대해 아주 스무스하게 재밌게 설명해주고 있다. 사실 이 부분은 수식을 보면 쳐다도 보기 싫은 구간인데, 굉장히 직관적으로 체계적으로 설명이 되어있다고 생각한다. (필자는 Restricted boltzmann machine 에 대해 여태 잘못 알고 있었음을,, 책을 읽으며 깨달았다.) </p> <p>3장과 4장에서 개념의 추상화와 구체화에 대해 설명하였고, 드디어 5장에서 시간축이 등장한다. 딱 들어맞는 비유와 함께 시간축이 필요한 이유, 모델의 구조에 대해 설명하고 있다. LSTM 관련 내용도 나오는데 책 읽다가 관심있는 사람은 구조를 실제로 검색해 보면서 책을 다시 읽어보길 바란다! 또 transformer과 self-attention 개념에 대해서도 간략히 소개하고 있다. </p> <p>프롤로그에서 교수님은 <strong>&quot;뇌와 인공지능의 작동원리는 같지 않다.&quot;</strong> 라고 얘기하며 시작하는데, 그와 관련된 내용이 6장에서 설명된다. 인공지능의 학습에 사용되는 error backpropagation 이 뇌에서는 쉽지않은 메커니즘임을 언급하고, biological plausible model 즉 생물학적으로 가능한 모델과 관련된 연구를 소개한다. 구체적으로 가장 기초적인 synaptic plasticity 부터 temporal error model 과 predictive coding 을 고요 속의 외침이라는 게임에 비유해서 설명한다. 마지막으로 최근 활발히 이루어지고 있는 global connectivity 와 관련된 apical dendrite 관련 연구를 소개하며 장을 마무리 한다. (이 쪽이 좀 더 자세히 소개되길 바랐는데 .. 한 장으로만 소개되어서 아쉬웠다 흑)</p> <p>마지막으로 7장에서는 god 이상완 교수님의 연구실에서 수행되고 있는 연구와 관련된 내용으로 이루어져 있다. Reinforcement learning 그 중에서도 model-free, model-based RL 에 대해 간단히 소개하고, 실제로 뇌의 특정 영역의 활성도가, RL setting 에서 reward, 두 module 사이의 weight 등의 component와 correlation 을 보인다는 연구에 대해 소개하고 있다! 그리고 책은 에필로그와 함께 마무리 된다.</p> <h2 id="---2">--</h2> <p>사실 책의 내용은 KAIST 의 Brain-Inspired Machine Intelligence 수업에서 자세하게 다루어진다. 필자는 2021년도 봄학기에 이 과목을 수강했기에 굉장히 익숙하면서도 반가웠다. (작년에 필기했던 lecture note 를 패드로 띄워놓고 책을 읽었다.) 나만 알고 싶었던 명강의 였는데 세상에 나와서 서운 반 기쁨 반 이랄까... <img src="https://velog.velcdn.com/images/jaeheon-lee/post/b62abb5f-3f55-4118-bf64-58e2445de1a6/image.png" alt=""></p> <p>이상완 교수님 랩에서 개별연구 할 때 기억도 새록새록 나고, 교수님을 비롯해서 연구실 선배님들께 여러가지로 좋은 영향과 도움을 받아서 지금은 어떻게 지내시나.. 연락을 먼저 해볼까 하는 그런 생각도 든다. 아! 이상완 교수님 랩 홈페이지는 aibrain.kaist.ac.kr 이다. (많관부) </p> <p>아무쪼록 카페 마감시간이 다 되어서 글은 여기까지 마무리 하려 한다. 책을 지원해주신 솔출판사 와 BCSC, 그리고 항상 존경하는 이상완 교수님께 감사인사를 전합니다! </p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; September 24, 2022 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Self-supervised-contrastive-learning-for-digital-histopathology" target="_blank">[Paper Review] Self supervised contrastive learning for digital histopathology</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="self-supervised-contrastive-learning-for-digital-histopathology">Self supervised contrastive learning for digital histopathology</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/3cf4dabc-cf49-4e48-b398-11df9e7309e0/image.png" alt=""></p> <h2 id="introduction">Introduction</h2> <p>SSL method의 다양한 hyperparameter를 바꾸고 histopathology에 적용하여 실험을 진행한 논문이다. SimCLR 을 사용했으며 downstream task - classification, segmetnation, regression - 을 통해 pretrain domain, pretrain image 개수, downstream 시 labeled image 비율, optimizer, resolution, augmentation (특히 color jittering 및 random resize crop), pretrain backbone architecture 등 다양한 실험 조건을 적용하여 결과를 리포트 하였다. </p> <h2 id="method">Method</h2> <p>SimCLR 에서 NT-Xent loss 를 언급하고 있다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/e2ab0e7c-ba60-411c-9672-7256bd5b1e6a/image.png" alt=""></p> <p>또한, contrastive method 는 image patch 간의 slient feature 를 학습하기에, pretraining 시 samples 이 같은 dataset으로부터 construct 되는 것 보다 다양한 dataset 으로부터 추출되는 것이 더 좋은 representation 을 학습할 수 있을 것이라 설명한다. (그리고 이를 증명한다.)</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/ffa4f669-741e-4b19-ba37-6d19b5fd589c/image.png" alt=""></p> <h2 id="experiments">Experiments</h2> <h3 id="pretraining-datasets">Pretraining datasets</h3> <p>길게 설명되어 있지만, supplement 의 table 하나로 설명된다. 총 57개의 dataset 중 22개는 image patch 로만 이루어진 dataset 이고, 나머지는 TCGA, CPTAC와 여러 challenge 에서 사용된 WSI 들로 구성되어 있다. 20x 부터 100x 까지 23개 장기에 대한 다양한 patch 와 slide 가 존재한다. (총 개수는 언급되지 않았음. 하지만 세어보면 대략 patch 4만장 + WSI 1만장 정도)</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/ef4813fc-e3f2-494b-8a17-06150faba7b4/image.png" alt=""></p> <h3 id="validation-datasets">Validation datasets</h3> <p>pretrain 후 다섯 개의 dataset 으로 classification, 두 개의 dataset 으로 segmentation, 하나로 regression task 를 진행하였다. classification - BACH (4, breast), Lymph (3, lymph), BreakHisv1 (2, breast), NCT-CRC-HE-100K (9, colorectal), Gleason2019 (5, prostate) segmentation - BACH (4, breast), DigestPath2019 (2, colon) regression - BreastPathQ (percentage cancer cellularity score to a given image patch, breast)</p> <h3 id="validation-tasks--setup">Validation tasks &amp; setup</h3> <p>COMPARE with 1) randomly initialized 2) ImageNet pretrained / in Resnet 18, 34, 50, 101 training settings: 1) fine-tuning, 2) last layer 제외 freeze(seg 제외) supervised learning ++ pretrained feature 를 이용한 clustering, feature selection ,,,</p> <p>100 epochs per experiment, Adam optimizer, batch size 128 training 50%, validation 25%, test 25%</p> <h2 id="results">Results</h2> <p>따로 모델 아키텍쳐가 쓰여있지 않으면 Resnet18로 학습된 결과이다. 50만장의 224 x 224 pixel image patch 를 1000 epoch 학습 시키는데 24시간 걸린다.</p> <h3 id="fine-tuning">fine-tuning</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/66ff35e5-3999-44cc-8b70-cb6c50d382bd/image.png" alt=""></p> <p>전반적으로 Self sup. setting 에서 가장 성능이 좋았다. 다만 Resnet 50, 101 segmentation 상황에서는 Imagenet pretrain 성능이 높았다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/36e464f0-5513-4efd-8907-dba22519a692/image.png" alt=""></p> <p>또한 supervised learning setting 에서 labeled data 가 &quot;적을 때&quot; SSL 성능이 다른 방법 (random=from scratch, Imagenet pretrained) 에 비해 크게 높았다. 저자는 NCT 를 예로 들며 설명하였다.</p> <h3 id="freeze">freeze</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/efb720cf-d015-4dc6-b3ea-63bf775b9f26/image.png" alt=""></p> <p>위 테이블의 모든 task, condition 에서 SSL 방식을 사용했을 때 성능이 가장 좋았다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/89b9cb32-76ac-4b42-b145-b110e2ddc31c/image.png" alt=""></p> <p>segmentation 의 경우 따로 freeze experiment 결과를 리포트 하지는 않았다. (since a decoder can contain millions of parameters that can be trained to achieve satisfactory performance regardless of the encoder weights.)</p> <h3 id="unsupervised-clustering-using-the-learned-representations">unsupervised clustering using the learned representations</h3> <p>learned representation can also be used for querying an image to its nearest neighbors without clustering the dataset, which is useful in applications such as active learning for sample selection and various data retrieval systems. </p> <p>randomly sampling patch 방식은 class imbalance 문제를 야기할 수 있기 때문에 representative patches 를 뽑기 위해 clustering 좋다. 특히 저자는 69 WSI 로부터 1.4 million image patch 를 뽑고, 3000개의 cluster로 데이터를 나누었다. (Elbow heuristic 방식을 이용해 1000, 1500,,, 10000 중 3000 선택) 그 방식으로 mini-batch K-means algorithm 을 사용하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/680810fb-80ea-4dce-b07f-d6e9bffe1c44/image.png" alt=""></p> <p>figure 6 에서, 약간의 cluster 를 뽑아 highlight 했더니 any supervision 없이 segmentation 도 어느정도 되었다고 주장한다.</p> <h3 id="pretrain-시-image-patch-개수와-resolution-바꿔서-실험">pretrain 시 image patch 개수와 resolution 바꿔서 실험</h3> <p>1) 0.01, 0.1, 1, 10% 의 image 만을 사용해 pretrain 2) IDCGrade (breast, 10x, 20x, 40x, 922 patches ??) 로 pretrain <img src="https://velog.velcdn.com/images/jaeheon-lee/post/abacabe5-5699-4a94-a226-af36ae7a4f2a/image.png" alt=""></p> <p>더 많은 데이터를 사용해 pretrain 할 수록 성능이 좋았고, 더 높은 resolution data로 학습했을 때 성능이 더 좋았다. 물론 다양한 resolution을 이용해 학습했을 때가 성능이 제일 좋았다.</p> <h3 id="transferability-of-features-between-tissue-types-and-staining">transferability of features between tissue types and staining</h3> <p>breast, lymph node, prostate 모두 4000개 정도씩 사용. breast: TCGA-PRAD, TUPAC16, TNBC, ICPR,,, / lymph nodes: Camelyon 16, 17 / prostate: Prostate-MRI</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/135888d1-0701-4ad7-8f98-0a51c91423e0/image.png" alt=""></p> <p>site-specific pretraining 에도 불구하고, pretrained model 과 validation performance 사이의 strong correlation 은 관찰되지 않았다. 또한, 가장 첫번째 table 의 결과 (77, 6.6, 74) 와 비교했을 때, 특정 domain 에 대해 실험한 결과보다 성능이 떨어졌다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/5048daa8-83c0-4a47-b575-1ac01ca8c1b3/image.png" alt=""></p> <h2 id="hyperparameter-and-suitable-augmentation-selection">hyperparameter and suitable augmentation selection</h2> <p>(여기부턴 supplment 에 나와있는 내용으로, 위 result 에 공통적으로 들어가는 setting 이다.)</p> <h3 id="optimizer-and-temperature">optimizer and temperature</h3> <p>pretraining 과정에서 Adam + batch size&gt;256 세팅으로 했을 때 converge 하지 않았고, Lars 가 Lamb 보다 성능이 조금 더 높게 나왔으며, smaller temperature 에서 학습이 전반적으로 더 잘 나오는 것을 확인하였다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d11cc487-a703-47d1-b9ca-abf8b32751d2/image.png" alt=""></p> <h3 id="augmentation">augmentation</h3> <p>사용한 augmentation 기법은, randomly resized crops, 90 rotation, flips, color jittering, Gaussian blurring 을 사용하셨고, 그 중 특히 color jittering 과 random resize crop 조건을 달리하여 실험을 진행하였다. color jittering 조건 light, medium, heavy 는 각각 (brightness, contrast, saturation, hue) 기준 (0.4 0.4 0.4 0.2) (0.8 0.8 0.8 0.2) (0.8 0.8 0.8 0.4) 을 의미한다. randomly resize crop 의 퍼센트는 그만큼의 patch size로 crop 했다는 뜻이다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/3898d657-dcca-4518-8931-640932356253/image.png" alt=""></p> <p>결과 더 강한 augmentation 이 적용될 수록 성능도 높아졌다. 심지어 1% random resize crop 은 224 x 224 pixel 기준 20 x 20 pixel 만 가져온 것임에도 불구하고, 다른 random resize crop 비율 실험보다 성능이 더 높았다...</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/0e98cda9-a171-4ff4-a703-1b70487a52de/image.png" alt=""></p> <h2 id="conclusion">Conclusion</h2> <p>방대한 양의 데이터셋과 여러 조건을 바꿔 SSL 로 실험한 노력이 대단한 .. 논문이었다. 논문에선 no prior research on histopathological image analysis with a ... consistently reaches or surpasses supervised training. 이라고 하는 걸 보면.. 정말 방대한 양을 썼긴 했나보다. 는 생각이 든다. SSL 방식을 사용할 때 어떤 setting 을 사용할지 고민하다가 찾은 논문인데 대신 여러 실험을 진행해줘서 참고할 때 좋을 것 같아 고마운 생각도 들었다. 암튼 굿. </p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; September 13, 2022 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Pan-cancer-integrative-histology-genomic-analysis-via-multimodal-deep-learning" target="_blank">[Paper Review] Pan-cancer integrative histology-genomic analysis via multimodal deep learning</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="pan-cancer-integrative-histology-genomic-analysis-via-multimodal-deep-learning">Pan-cancer integrative histology-genomic analysis via multimodal deep learning</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/6d4be227-bbe9-404f-8232-628ec7089a26/image.png" alt=""></p> <p>CLAM 으로 익숙한 하버드의 mahmood lab 의 연구가 얼마 전에 공개되었다. TCGA data의 14개 암종에 대해, diagnostic tissue image 와 molecular profile data 를 융합하여 feature 를 만들어내었고 이를 survival analysis 에 활용하였다. c-index 등의 ranking performance 뿐만 아니라 interpretability 면에서도 훌륭한 성능을 보여주었다. </p> <h2 id="introduction">Introduction</h2> <ul> <li>recent DL-based approaches는 survival label 과 같은 outcome-based label 을 활용하여 objective and prognostic molecular feature 를 찾고자 함.</li> <li>joint image-omic biomarker ex) oligodendroglioma and astrocytoma histolgoies with IDH1 mutation and 1p/19q-co-deletion status 는 정교한 patient stratification 가능케 함.</li> <li>multimodal fusion 은 improve precision &amp; assist discover biomarker</li> </ul> <h2 id="method">Method</h2> <ul> <li>joint image-omic biomarker 를 발굴하기 위해, MMF (multimodal fusion) algorithm 을 제안함.</li> <li>MMF: H&amp;E WSI, molecular profile feature (mutation status, copy-number variation, RNA seq)</li> <li>survival outcome prediction 을 수행함과 동시에, how histopathology/molecular features + their interaction 이 low-, high-risk patient 와 correlation 을 이루는지 분석 수행.</li> <li>1) attention-based 2) attribution-based method 를 통해 explainability 확보.</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/1573d8a9-89c2-48bd-9527-484062096662/image.png" alt=""></p> <p>1) attention-based Multiple Instance Learning for processing WSI 2) Self-Normalizing Networks (SNN) for processing molecular profile data 3) multimodal fusion layer for tegrating WSIs and molecular profile data</p> <h3 id="dataset-description--wsi-preprocessing">dataset description &amp; WSI preprocessing</h3> <p>TCGA project 의 14개 암종, 5720명 환자의 데이터 public CLAM 의 repository 를 활용하여 automated tissue segmentation 진행 image patch size 256 x 256, ResNet50 pretrained by ImageNet -&gt; 1024 dimensional feature vector</p> <h3 id="amil">AMIL</h3> <p>따로 clustering 기법 등을 사용하지 않은, AMIL를 처음 제안한 논문의 방법을 인용하고 사용함. WSI processing 후 WSI bag 은 $M_i \times C$ , (patch 개수 X 1024) 로 표현됨. 환자 수를 N 이라 했을 때 attention pooling 의 대상이 되는 matrix 는 $M_i \times C \times N$ 이고, 이를 다음 식을 통해 $512 \times N$ 로 변환. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/d544d939-37fb-4a97-aca3-2221fb32c190/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/daa362fd-3f67-4f7d-8b1a-2807820d4a76/image.png" alt=""></p> <p>자세한 설명은 생략하지만, 직관적으로 설명하면, WSI 의 patch 여러 개의 feature vector 들을, feature vector 의 관계와 중요도를 판단하는 학습 가능한 network 를 통해, 하나의 feature vector 로 aggregate 해주는 계산임. 이렇게 생성된 vector 는 $W_{pred} \in R^{4 \times 512}$ 와 sigmoid activation 을 거쳐, negative-log-likelihood function for discrete time survival modling 에 활용된다. 또한 last fully-connected layer 는 WSI representation $h_{WSI} \in h^{32 \times 1}$ 을 위해 사용되고 이는 multimodal fusion layer 에 활용된다.</p> <h3 id="snn">SNN</h3> <p>FCN 의 variation 중 하나로, high-dimensional low-sample size (HDLSS) sscenenarios 를 배우기 위한 architecture 중 하나이다. 자세한 내용은 모르지만 HDLSS property 를 띄는 molecular data 를 $h_{molecular} \in h^{32 \times 1}$ 로 encoding 하기 위해 사용된다. 이는 multimodal fusion layer 에 활용된다.</p> <h3 id="multimodal-fusion-layer">Multimodal fusion layer</h3> <p>두 representation $h_{WSI}$, $h_{molecular}$ 의 모든 bimodal interaction 을 잡아내기 위해, Kronecker Product 를 사용하여 새로운 differentiable fusion tensor $h_{fusion} \in h^{33 \times 33}$ 을 계산한다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/98012ffb-492d-495c-9435-83af729e9bf2/image.png" alt=""></p> <p>이 때, unimodal feature 도 잡아내고 서로 다른 modality의 feature collinearty 를 줄이기 위해 bias 처럼 1 을 concat 해준 뒤 Kronecker Product 를 계산하였다. 또한 이와 더불어, gating-based attentnion mechanism 을 추가적으로 적용하여, 각 modality 의 expressiveness 를 control 하고자 하였다. (i는 각 modality)</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/fa5249b1-413b-4e09-881a-9b5111a5322a/image.png" alt=""></p> <p>각 modality representation vector 에 대해 gated mechanism 을 먼저 적용한 후 fusion matrix 를 계산하였다. 이후 size 256의 두 개의 hidden layer 를 거쳤고, survival analysis 를 위한 cross entropy 처럼 생긴 loss function 을 계산하여 모델을 업데이트 하였다. </p> <h2 id="results">Results</h2> <h3 id="model-performances-of-porpoise-and-understanding-impact-of-multimodal-training">model performances of PORPOISE and understanding impact of multimodal training</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/2a3a675c-c8ce-43f8-bd92-340c7b1e6032/image.png" alt=""></p> <p>분석 대상이었던 모든 암종에 대한 성능 정보를 한 눈에 볼 수 있는 figure 이다. KM curve 를 봤을 때, 14개 중 4개 (HNSC, LIHC, LUSC, STAD) 를 제외하고 11개에 대해 significance 를 얻었다. c-index 의 경우 14개 중 2개 (LIHC, UCEC) 를 제외하고 12개에 대해 SNN, AMIL 즉 unimodality 정보만 사용한 것 보다 MMF 의 c-index 가 더 뛰어났다. fusion layer 에 들어가기 전 각 modality feature 에 대해 gate layer 를 통해 각 modality 의 attribution 을 계산하여 나타낸 결과, 대부분의 암종에 대해 molecular feature 의 지분이 압도적으로 컸지만, 그 중 LIHC, STAD, UCEC 는 WSI attribution 이 높게 나타났다. </p> <h3 id="quantitative-performance-local-model-explanation-and-global-interpretability-analyses-of-porpoise-on--cancer">Quantitative performance, local model explanation, and global interpretability analyses of PORPOISE on ... cancer</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/a5b07c9e-5f71-4eda-9d05-0c9f29335035/image.png" alt=""></p> <p>위와 같은 분석 자료가 supplementary data 를 포함하여 14개 암종 모두에 대해 주어졌다. Whole Slide Image 를 patch 화 하고 AMIL 의 attention score 를 이용하여 global 한 heatmap 과 ROI heatmap 를 A 에 나타내었다. B, D 는 SHAP 을 이용한 feature attribution visualization 결과이다. 각 모델에서 어떤 feature 가 의사 결정에 얼마나 영향을 미쳤는지를 알려준다. 예를 들어 B 는 local interpretability, 즉 individual sample 의 각 feature 가 얼마나, 어떻게 model risk prediction 에 영향을 미쳤는지를 보여주는데, x axis 는 attribution value 로 얼마나 에 해당하고 y axis 는 attribution 을 sorting 한 것이다. continuous colormap 은 해당 gene 이 mutation (1) 일 때 영향을 주는지 wildtype (0) 일 때 영향을 주는지를 나타내는 지표이다. D 는 global interpretability, 즉 해당 cancer cohort 전반에 대해 어떤 feature 가 의사결정에 얼마나 어떻게 영향을 미쳤는지를 보여준다. 자세한 내용은 SNN 쪽이라 일단은 남겨두도록 하자. C는 unimodality 를 사용했을 때와 fusion을 사용했을 때 각각 score 를 구한 뒤 KM curve 를 나타낸 결과이다. 전반적으로 unimodality 만을 사용했을 때 보다 multimodality 를 사용했을 때 두 group 이 더 잘 나뉘고 logrank test 결과 p value도 더 signifcant 하게 계산되었다. 마지막으로 E 는 attention score 가 높은 상위 1% patch (약 135개 정도) 만을 추출하여, HoverNet 을 이용해 cell segmentation 을 수행한 결과이다. 이 결과를 정리하여 cell quantification 까지 수행하였고, 특히 대부분의 암종에 대해 TIL (tumor infiltrating lymphocyte) 관련한 유의미한 결과를 얻었다. </p> <p>다음부터는 논문에서 언급한 중요 결과의 요약이다.</p> <ul> <li>12/14 cancer type 에서 MMF 는 highest c-index 달성</li> <li>7/14 cancer type 에서 MMF 는 AMIL 보다 stratification significance 를 더 확보함.</li> <li>Kronecker product 를 이용한 fusion 을 사용했을 때 c-index 가 제일 잘 나옴.</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/05f92928-6d4e-45e0-b257-ecac66cb9a64/image.png" alt=""></p> <ul> <li>8/14 cancer type 에서 low/high risk group 간 lymphocyte cell fraction 의 significant difference 를 보임.</li> <li>6/14 cancer type 에서 low/high risk group 간 tumor cell fraction 의 significant difference 를 보임.</li> <li>LGG cohort 에서 IHD1 mutation 이 아주 중요한 prognostic factor 로 계산되었고, 이는 기존 연구들과 consistent 함.</li> <li>PAAD cohort 에서 전반적으로 innate immunity 와 inflammatory cell signaling 에 관여하는 gene 이 중요하다 계산됨. </li> <li>tumor-infiltrating lymphocyte (TIL) presence 가 9/14 cancer type 에서 significant difference 를 보임.</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/367e5fb8-e6e7-4460-9cf7-ba6706b1013c/image.png" alt=""></p> <p>이때, TIL presence 는 highest-attended patch 에서 high tumor-immune cell co-localization 여부 (저자의 heuristic) 를 이용해 판단하였다. 20개의 cell 이 있고, 10개 이상의 lymphocyte 가 존재하고, 5개 이상의 tumor cell 이 존재할 때 TIL 이 존재한다 라고 정의하였다. 저자는 한계점에 TIL presence 와 statistical significance 를 평가하기 위해 post hoc analyses 가 필요하다 라고 덧붙였다. </p> <h2 id="discussion">Discussion</h2> <p>PAAD 의 경우, AMIL 이 prognostic 하지 않다 계산되었고 SNN 보다도 성능이 좋지 않았지만, multimodal integration 을 통해 성능이 향상됨과 동시에, WSI의 attribution score 비교적 높았다. 그 반대로 BRCA, COADREAD, LUAD 의 경우, uni-modality 일 때보다 multimodal integration 을 통해 성능이 향상되었고 WSI 대신 molecular feature 의 attribution score 가 높았다. 앞선 결과들을 포함한 논문의 전반의 설명을 증거로, computational support system for therapeutic decision-making 을 통해 향후 genotype-phenotype correlation-based analyses 는 shared + modality-specific 한 prognostic information 을 찾고 single/joint biomarker 를 발굴하는데에 큰 도움이 될 것이라 주장한다.</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; August 22, 2022 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Bias-in-Cross-Entropy-Based-Training-of-Deep-Survival-Networks" target="_blank">[Paper Review] Bias in Cross-Entropy-Based Training of Deep Survival Networks</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="bias-in-cross-entropy-based-training-of-deep-survival-networks">Bias in Cross-Entropy-Based Training of Deep Survival Networks</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/de674758-6443-4193-a779-2b134e71d8fb/image.png" alt=""></p> <p>Deep survival networks 에서는 크게 두 분류의 loss function 을 사용하여 타겟 네트워크를 optimize 한다. 첫번째로는 proportional hazard model 로부터 유도된 DeepSurv 에서 최초로 사용한 negative log likelihood 계열 loss 가 있고, 두번째는 event time 을 discretize 하여 fixed boundary 를 이용한 classification 계열 loss 가 있다. 본 논문에서는 survival analysis 에서 기존 사용되던 cross-entropy loss 에 특정 bias 가 끼여있음을 이론, 실험적으로 증명하고 이를 완화한 새로운 loss 를 제안하고 성능을 비교 분석한다. </p> <h2 id="introduction">Introduction</h2> <p>Discrete hazard function 과 discrete survival function 에 대해 먼저 설명하고 있다. 다음과 같이 정의된다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/5ad27610-8e18-47dd-8d0b-378f788c4442/image.png" alt=""></p> <p>여기서 설명을 놓치면 앞으로도 계속 이해가 안갈테니 꼼꼼하게 짚고 넘어가는 것이 좋다. T 는 환자들의 event time 을 categorical value 로 만들어 놓은 discrete time variable 이다. 예를 들어 t_1 을 0 부터 100일 이런식으로 잡을 수 있는 것이다. X 는 환자의 time, event 정보 이외의 age, stage 등 환자의 survival 을 추정하기 위해 사용되는 variables 이다. h(t|X) 는 해당 환자가 (조건부) X 라는 variable 을 가지고 있고 t 라는 시간대까지 살아남았을 때, t 라는 시간대에 죽을 (event 가 일어날) 확률 이다. 이 때, 1-h(t|X) 는 그 여집합으로, 동일 조건에서 t 라는 시간대에 죽지 않을 확률로 해석할 수 있다. S(t|X) 는 t 라는 시간대에 죽지 않았을 확률 이다. 즉 이를 다시 표현하면, 첫번째 시간대부터 t-1 시간대, t 시간대에 죽지 않을 확률의 joint probability 로 볼 수 있다. 결국 deep survival network 를 통해 위 환자마다 계산된 output layer 의 real-valued output 로부터 hazard function h(t|X) 오른쪽 삼지창 (0,1] 를 추정하는 것이 목적이다. </p> <p>그렇다면 기존 사용되던 cross-entropy loss 는 어떤 형태를 띄고 있을지가 궁금해진다. survival analysis 는 right-censored data 이기 때문에, censored data 와 uncensored data 가 다르게 계산되어 loss function 에 들어가야 할 것이다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/1ba9b810-8614-4ad5-85e6-350b1e81041a/image.png" alt=""></p> <p>(xi, ti) 는 환자 한 명의 정보이다. ci 는 censored 되었는지 즉 event 가 일어났는지를 의미한다. censored data, event 가 일어나지 않은 patient 는 ci = 1 로 표기되기 때문에, ci 가 앞에 붙은 항은 event 가 일어나지 않은 patient 에 대해서만 계산되는 항이다. 반대로 (1-ci) 가 앞에 붙은 항은 event 가 일어난 patient 에 대해서만 계산되는 항이다. 위 식의 ci 가 붙은 항 S(ti|xi) 는, &quot;event 가 일어나지 않은 patient 가, ti 에 안 죽어 있을 확률&quot; 이다. 그리고 (1-ci) 가 앞에 붙은 항 (1-S(ti|xi)) 은 &quot;event 가 일어난 patient 가, (언제 죽었는지는 모르지만) ti 에 죽어있을 확률&quot; 이다. 논문의 저자는 이 (언제 죽었는지는 모르지만) 때문에 bias 가 낀다 설명한다. 그 환자는 t_i 시간대에 딱 죽은 환자인데, loss 에는 언제 죽었든 t_i 에 살아있기만 하면 계산이 되지 않도록 설계되어 있기 때문이다.</p> <h2 id="theoretical-analysis-of-the-cross-entropy-loss">Theoretical Analysis of the Cross-Entropy loss</h2> <h3 id="comparison-of-the-cross-entrop-loss-and-the-negative-log-likelihood-loss">Comparison of the Cross-Entrop loss and the Negative Log-Likelihood loss</h3> <p>저자는 다른 논문의 discrete survival model 에서 사용된 log-likelihood function 을 벤치마킹하여 loss 를 설계한다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/21e806c9-e321-4831-83b0-396efc9f7aae/image.png" alt=""></p> <p>(1-ci) event 가 일어난 환자가 ti 라는 시간대에 딱 사망할 확률 + ci event 가 일어나지 않은 환자가 ti 라는 시간대에 생존해 있을 확률 이다. 이를 이리저리 잘 바꾸고 negative log likelihood 꼴로 만들어 다음과 같은 loss 로 변형한다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d1d722a0-319b-476d-a728-c37cdde5aa4c/image.png" alt=""></p> <p>ci event 가 일어나지 않은 환자가 ti 시간대에 생존해 있을 확률 + (1-ci) event 가 일어난 환자가 ti-1 시간대까지 계속 생존해 있을 확률 + (1-ci) event 가 일어난 환자가 ti 시간대에 딱 사망할 확률 로 해석할 수 있다. </p> <p>기존 사용되던 cross-entropy loss 와 새롭게 고안한 negative log-likelihood loss 의 차를 계산하면 다음과 같다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/708c462d-19cd-4eed-9981-324b09013baf/image.png" alt=""></p> <p>크게 봤을 때 두 loss 의 차이는 (1-ci), event 가 일어난 환자들에 의해 일어난다. 만약 모두 censored 되어 모든 환자의 ci=0 이라면 두 loss 는 같을 것이다. 좀 더 구체적으로 보면, (1-S(ti|xi) 가 S(ti-1|xi) * h(ti|xi) 에 가까워 질수록 두 loss 의 차이는 줄어든다. 이 두 식을 해석하기 편하게 정리하면 다음과 같다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/e0297e90-4f4f-4238-b324-f880e1c09a7b/image.png" alt=""></p> <p>첫번째는 ti 에 이미 환자가 죽어있을 확률을, ti 에 죽을 확률과 ti 이전에 죽을 확률로 나눈 것이다. 그 아래의 hazard function 은 조건부를 분모로 보낸 것이다. 두 차이는 P(T &lt; ti|xi) 로 정리되고, 이 식이 0으로 가면 두 loss 의 차이가 0 으로 간다. 결국 event 가 일어난 환자에 대해, 모든 ti 에 대해, P(T &gt;= ti | xi) = 1 일수록 차이가 줄어든다는 뜻이고, 이를 다시 해석하면, &quot;large event times for all instances&quot; 를 뜻한다. 하지만 large event times 를 가진 환자는 (보통 오래 추적하기 어려우므로) 대게 censoring rate 가 높아 censoring rate 가 낮으면 어쩔 수 없이 발생하는 오차라고 설명한다. </p> <h3 id="bias-in-cross-entropy-based-hazard-estimates">Bias in Cross-Entropy-Based Hazard Estimates</h3> <p>cross-entropy loss 로부터 발생하는 bias를 설명하기 위해, constant hazard function h(t|X)=h for all t 를 잡고 simulation 한다. 또한 censoring rate 를 0 부터 1까지 다르게 하여, 어떻게 추정치가 달라지는지를 plot 한다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/8c3c665e-ff96-4cc2-8b71-94054388a5ff/image.png" alt=""></p> <p>결과, 계산대로 censoring rate 가 낮을수록 (event 가 일어난 환자가 많을수록) cross-entropy 와 negative log-likelihood 사이 큰 차이가 존재했고, censoring rate 가 1 에 가까워 질수록 그 차이는 줄어들어 갔다. 또, cross-entropy loss 를 사용했을 때 hazard rate의 overestimation 과 survival function 의 underestimation 을 관찰하였다. </p> <h2 id="empirical-evidence">Empirical Evidence</h2> <p>이론적으로 difference 및 bias 가 존재함을 보이고, 실제 사용되는 deep survival network (DRSA network, DeepHit network) 에서 사용되는 cross entropy loss 를 고안한 negative log-likelihood loss 로 대체하여 실험을 진행하였다. c-index와 calibration 정도를 나타내는 GND test, survival function과 KM curve 의 비교를 통해 performance 를 비교하였다. 또한 다음과 같은 regularization term 앞에 alpha 를 곱해 두 loss 에 추가하여 성능을 비교하였다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/a17ae725-be14-4b12-b0cb-e70684d9dc8c/image.png" alt=""></p> <p>이 alpha 가 커질 수록, uncensored data 에 더 weight를 크게 한 loss 가 계산되어 network에 update 된다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/3a42fa80-981e-459a-a560-e2600a5eaac1/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/ada1e397-bda1-461e-b0aa-31efed1d0726/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/42c32c66-a93a-49be-b67d-d7a4ce76ca59/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/92f8b340-2a9b-42c6-bede-15ee284f19ca/image.png" alt=""></p> <p>다음과 같은 결과를 얻었고, alpha 는 전반적으로 좋은 결과를 내었지만 특히 unbalance 된 데이터 일 때 좋은 결과를 얻었다. GND test 결과는 크면 클수록 덜 calibration 된, 즉 bias 가 껴있음을 나타내는데, 새롭게 고안된 loss 와 alpha 를 적용했을 때 calibration 이 훨씬 잘 됨을 보여준다. 또한 ranking performance 또한 전반적으로 개선되었다. </p> <h2 id="conclusion">Conclusion</h2> <p>본 논문에서 진행한 theortical / empirical analysis 결과를 통해, cross-entropy-based training of deep survival network 는 large prediction error 과 함께 bias 를 초래함을 증명하였다. 이와 반대로 새롭게 고안한 loss 를 적용했을 때 better calibrated prediction rule 을 보였고, predicted survival probabilities 에서 smaller bias 와 함께 reduce prediction error 함을 보였다. </p> <p>cf) 구현도 매우 간단했다..! --&gt; ce_l ㄱㄱ <img src="https://velog.velcdn.com/images/jaeheon-lee/post/13e786ac-abf3-45fd-aaa0-2fea2156e821/image.png" alt=""></p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; August 15, 2022 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p></li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/4/" tabindex="-1" aria-disabled="4">Newer</a> </li><li class="page-item "><a class="page-link" href="/blog/page/4/index.html" title="blog - page 4">4</a></li> <li class="page-item active"><a class="page-link" href="/blog/page/5/index.html" title="blog - page 5">5</a></li> <li class="page-item "><a class="page-link" href="/blog/page/6/index.html" title="blog - page 6">6</a></li> <li class="page-item "><a class="page-link" href="/blog/page/7/index.html" title="blog - page 7">7</a></li> <li class="page-item "><a class="page-link" href="/blog/page/8/index.html" title="blog - page 8">8</a></li> <li class="page-item "> <a class="page-link" href="/blog/page/6/">Older</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> &copy; Copyright 2023 JaeHeon Lee. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>