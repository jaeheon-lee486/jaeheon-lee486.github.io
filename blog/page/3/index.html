<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>blog - page 3 | JaeHeon Lee</title> <meta name="author" content="JaeHeon Lee"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🧠</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jaeheon-lee486.github.io/blog/page/3/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">JaeHeon&nbsp;</span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <div class="header-bar"> <h1>al-folio</h1> <h2>a simple whitespace theme for academics</h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>&bull;</p> <li> <i class="fas fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>&bull;</p> <li> <i class="fas fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <br> <div class="container featured-posts"> <div class="row row-cols-2"> <div class="card-item col"> <a href="/blog/2021/distill/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a distill-style blog post</h3> <p class="card-text">an example of a distill-style blog post and main elements</p> <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp; <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> </p> </div> </div> </div> </div> </a> </div> <div class="card-item col"> <a href="/blog/2015/code/"> <div class="card hoverable"> <div class="row g-0"> <div class="col-md-12"> <div class="card-body"> <div class="float-right"> <i class="fa-solid fa-thumbtack fa-xs"></i> </div> <h3 class="card-title text-lowercase">a post with code</h3> <p class="card-text">an example of a blog post with some code</p> <p class="post-meta"> 4 min read &nbsp; &middot; &nbsp; <a href="/blog/2015"> <i class="fas fa-calendar fa-sm"></i> 2015 </a> </p> </div> </div> </div> </div> </a> </div> </div> </div> <hr> <ul class="post-list"> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-DTFD-MIL-Double-Tier-Feature-Distillation-Multiple-Instance-Learning-for-Histopathology-Whole-Slide-Image-Classification" target="_blank">[Paper Review] DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="dtfd-mil-double-tier-feature-distillation-multiple-instance-learning-for-histopathology-whole-slide-image-classification">DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/442c4d14-8171-4395-baa2-84c699b3d18e/image.png" alt=""></p> <p>특정 dataset 에 대해, MIL SOTA 칭호를 얻은 논문이다. 다른 논문에서 SOTA 로써 인용되기도 하고 아이디어도 reasonable 해보이고 코드도 공개되어 있어서 가져다 써보려 한다. (이미 누가 돌려봤다) 핵심 아이디어는 psudo-bag 을 통해 number of bag 을 늘린 것, 늘려 발생한 문제를 완화하기 위해 double-tier 구조를 착안한 것에 있다. </p> <h2 id="introduction">Introduction</h2> <ul> <li>WSI 분석과 MIL 등장 배경</li> <li>ABMIL 관련 서술</li> <li>instance probability inference 가 infeasible 하다 여겨져 왔고, 차선책으로 attention score 를 사용해옴.</li> <li>본 논문에서는 ABMIL framework 에서 instance probability 를 derive 함.</li> <li>mutual-instance relation 과 over-fitting problem 간에 trade-off 가 있음.</li> <li>negative impact 를 완화하기 위해 pseudo-bag 을 만듦.</li> <li>이로부터 발생하는 risk 를 완화하기 위해, Grad-CAM 을 활용한 double-tier MIL 구조를 고안함.</li> </ul> <h2 id="method">Method</h2> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/55f134f7-2e94-4e72-a082-dc8b1d3f7233/image.png" alt=""></p> <h3 id="revisit-grad-cam-and-ab-mil">Revisit Grad-CAM and AB-MIL</h3> <h4 id="grad-cam">Grad-CAM</h4> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/21f7e061-cdfe-481d-9f0a-cac6a32ccb2d/image.png" alt=""></p> <p>end-to-end image classification 에서 DCNN 의 high level feature map DxWxH (D: number of channel, W,H dimension)을 global average pooling 을 거쳐 channel D dimension 을 가진 feature 를 얻는다. 이 때 f 를 MLP 에 통과시키고 logit $s^c$ for class c (1,2,...,C) 를 얻는데, 아래 식을 거쳐 Grad-CAM의 class activation map 을 얻을 수 있다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b0b93913-cd51-4de2-9d9c-4caf4615ba34/image.png" alt=""></p> <p>즉, softmax 와 input f 에 의해 결정된 signal strength 로써 image 가 각 class C에 해당할 probability 를 계산하고, 아래 식과 같이 특정 위치에서 특정 class 에 대해 얼마나 신호가 강한지를 나타낼 수 있게 된다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/c927a2b8-8450-42bf-9e50-6b3a8c70ca37/image.png" alt=""></p> <h3 id="attention-based-multiple-instance-learning">Attention-Based Multiple Instance Learning</h3> <p>앞선 포스팅에서 많이 언급되었던 내용이라 간단히 수식들만 언급하도록 하겠다. Bag label - bag representation, bag classification task <img src="https://velog.velcdn.com/images/jaeheon-lee/post/f0457347-d604-4c29-85de-bc9759d05340/image.png" alt=""></p> <p>$G$ is an aggregation function, $h_k$: extracted feature for instance $k$ <img src="https://velog.velcdn.com/images/jaeheon-lee/post/dda8fbe0-ca09-44d1-89d4-fc1702aa0886/image.png" alt=""></p> <p>$a_k$ learnable scalar weight for $h_k$, D: dimension of vector $F$ and $h_k$ <img src="https://velog.velcdn.com/images/jaeheon-lee/post/ce78109e-0ba3-451d-928e-28c6a7de423a/image.png" alt=""></p> <p>weight from the classific AB-MIL, where $w, V_1, V_2$ are the learnable parameters <img src="https://velog.velcdn.com/images/jaeheon-lee/post/87c80dbc-89db-4a73-94d9-48af00a0498f/image.png" alt=""></p> <h3 id="derivation-of-instance-probability-in-ab-mil">Derivation of Instance Probability in AB-MIL</h3> <p>본 논문에서 핵심 내용 중 하나이다. 위에서 언급해왔듯 attention score 을 class probability 로 대신 사용해왔고, 이를 극복하고자 AB-MIL framework 가 가지는 특수성을 기반으로 individual instance 의 predicted class probaility 를 유도한다. 다음 proposition 위에서 derivation 이 이루어진다.</p> <p><strong>Proposition 1</strong> The paradigm of AB-MIL is a special case of the framework of the classic deep-learning network for image classification </p> <p>supplmentary 에 proposition1 의 증명과 설명이 있다. 저자의 논리는 이 proposition 1 에 기초하여, Grad-CAM 의 mechanism 을 AB-MIL 에 바로 적용하여, 각 instance 의 signal strength 를 Grad-CAM과 같이 derive 할 수 있다고 한다. </p> <p>supplementary 의 증명이라고 하기엔 애매한 설명은 다음과 같다. </p> <ul> <li>$F = \sum_{k=1}^Ka_kh_k = \frac{1}K\sum_{k=1}^Ka_kKh_k=\frac{1}K\sum_{k=1}^K\hat{h}_k$ : AB-MIL 은 averaging pooling 으로 볼 수 있다.</li> <li>$f =\frac{1}{W,H}\sum_{w,h}^{W,H}u_{w,h}$ : deep learning 에서의 GAP도 다음과 같이 쓸 수 있다</li> <li>deep learning 과 MIL 모두 넓게 보면 classification problem 이다</li> <li>F 와 f 의 유일한 차이는 spatial relation 이 F에는 고려되지 않았다는 점이다.</li> <li>하지만 Grad-CAM 에서 attention map inference 는 spatial relation 이 아무 역할을 하지 않는다</li> <li>그러므로, DCNN 에서 signal strength 를 나타내는 Grad-CAM 식 처럼 </li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b0b93913-cd51-4de2-9d9c-4caf4615ba34/image.png" alt=""></p> <ul> <li>AB-MIL 에서 각 instance 의 signal strength 나타내는 Grad-CAM 식을 아래처럼 쓸 수 있다</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/caef0deb-b182-41a3-b948-cc2b079b5844/image.png" alt=""></p> <p>보면, U 가 h 로 바뀌었고, $S^c$ 는 output logic for class c from MIL classifier 이다. 이 식에 softmax 를 적용하여 p를 구한다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/e1d9186a-0ad7-40b2-93f6-557829d210a9/image.png" alt=""></p> <p>supplementary 에 나온 설명을 덧붙이자면, 이 식은 instance 가 large enough attention score 를 가지고 있을 때만 유효한 값을 가진다. 그 이유는, certain patch 가 attention module 에서 deactivate 되어 $a_k$값이 0에 가까워지면, 그에 해당하는 $\hat{h}_k$도 0에 가까워지고, 이에 따라 $L_k^c$는 모든 class에 대해 0, $p_k^c$ 는 정보량이 없는 0.5에 가까워 질 것이기 때문이다. 이런 점 때문에 pseudo-bag 을 만들고 attention score 에 따른 patch selection 사전작업이 이루어 졌다는 점을 언급하고 있다.</p> <h3 id="double-tier-feature-distillation-multiple-instance-learning">Double-Tier Feature Distillation Multiple Instance Learning</h3> <p>double-tier feature distillation MIL framework 에 대해 자세히 언급한다. 여기부터는 크게 어려운 부분은 없다. 다만 notation 이 많아서 좀 헷갈린다. 그림 보면서 보자.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/442c4d14-8171-4395-baa2-84c699b3d18e/image.png" alt=""></p> <ul> <li>Given N bags (slides), each bag there are Kn instances (k=1,2, ..., Kn), (n=1, ..., N)</li> <li>i.e., $X_n = {x_{n,k}|k=1,2,...,K_n} ,n\in {1,2,...,N}$ with ground truth of bag $Y_n$</li> <li>feature of a patch, denoted as $h_{n,k}=H(x_{n,k})$</li> <li>randomly split into $M$ pseudo-bags, $X_n ={X_n^m | m=1,2,...,M}$</li> <li>inTier-1 model (AB-MIL model), estimated bag probability of pseudo-bag: $y^m_n$</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/f4128cf1-6f3a-41be-b3d0-772d794dc1a9/image.png" alt=""></p> <ul> <li>loss: cross-entropy L1</li> <li>in Tier-2, feature from each pseudo-bag is distilled, as $\hat{f}_n^m$</li> <li>이때, 참고로 pseudo-bag m개에서 나온 feature 가 aggregation 되었다.</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/ae7f6fd8-657d-462f-a33d-606a990c4f58/image.png" alt=""></p> <ul> <li>이 또한 cross-entropy L2</li> <li>overall optimization process 는 L1+L2 꼴</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/87123ba6-b621-4961-a5e3-c4b2f99b51c6/image.png" alt=""></p> <ul> <li>aggregation function 은 다음 4가지로 구성됨 1) Maximum selection 2) Maximum &amp; Minimum selection 3) Maximum attention score selection 4) Aggregated feature selection (attention 써서)</li> </ul> <h2 id="experiments">Experiments</h2> <h3 id="datasets">Datasets</h3> <p>two public histopathology WSI dataset</p> <ul> <li>CAMELYON-16</li> <li>TCGA lung cancer</li> <li>OTSU tissue, non-overlapping 256x256 patch on 20X mag, 3.7M + 8.3M patches</li> </ul> <h3 id="performance-comparison-with-existing-works">Performance comparison with existing works</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/35064373-1f14-46fe-9ea8-899ad64239b7/image.png" alt=""></p> <p>CAMELYON-16은 대부분 슬라이드에서 small portion of tumor 이고, MaxS 방식이 가장 inferior 했지만 여전히 다른 모델에 비해서는 성능이 좋았다. DTFD-MIL (AFS) 는 4% better on AUC.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/a58b74f2-c375-4975-a809-958204220cad/image.png" alt=""></p> <p>TCGA-lung cancer 에서 AUC 96.1% 를 보여주었다. 여기선 attention 을 쓰지 않은 instance-level feature 의 MaxMinS 방식에서 성능이 제일 높았는데, 이를 TCGA 의 large tumor region 가 원인이라 설명한다. </p> <h3 id="visualization-of-detection-results">Visualization of Detection Results</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/c44604ba-6797-4d6a-aae1-9e26fb003cc0/image.png" alt=""></p> <p>오호 Grad-CAM probability 써서 그런지 확실히 기존 attention score 쓰던거보다 낫다. 논문에서도 비슷하게 얘기한다.</p> <h2 id="conclusion">Conclusion</h2> <ul> <li>derivation of instance probability under framework of AB-MIL</li> <li>qualitatively demonstrate the derived instance probability was more reliable</li> </ul> <p>survival 에 붙여서 빨리 코드 돌려봐야 하는데... 다른 장기에서도 통했으면 좋겠다.</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; April 15, 2023 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Exploring-Visual-Prompts-for-Whole-Slide-Image-Classification-with-Multiple-Instance-Learning" target="_blank">[Paper Review] Exploring Visual Prompts for Whole Slide Image Classification with Multiple Instance Learning</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="exploring-visual-prompts-for-whole-slide-image-classification-with-multiple-instance-learning">Exploring Visual Prompts for Whole Slide Image Classification with Multiple Instance Learning</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/bb7a88f5-20cb-47cf-b9bb-f501a9a13242/image.png" alt=""></p> <p>훈련소에 다녀왔다. 3주 다녀왔는데 잘 안 읽히고 바보가 된 기분이다. 그래도 해야지. 이번 논문은 WSI analysis 에서 자주 쓰이는 Multiple Instance Learning 분야의 나온지 얼마 안된 따끈따끈한 논문이다. 병리 쪽에서는 natural image (ex ImageNet) 로 pretrain 된 모델을 backbone 으로 사용하는 경우가 많은데, 이러한 pretrained model 을 promopt learning 방식으로 비교적 적은 cost 로 fine-tuning 해서 MIL 성능을 높였다는 연구이다.</p> <h2 id="introduction">Introduction</h2> <ul> <li>WSI 분석에 Multiple Instance Learning (MIL) 많이 씀</li> <li>frozen feature extractor pretrain 해서 사용하지만, overlook domain shift issue</li> <li>fine-tuning feature extractor 는 large-scale dataset 에서 학습된 모델을 손상할 수 있음.</li> <li>NLP에서 영감받은 prompt learning 을 병리에 적용하여 improve performance, achieve domain transformation</li> </ul> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/945b2e10-5976-47f1-ad83-455ee263a35a/image.png" alt=""></p> <p>how?</p> <p>1) introduce visual promopts into WSI classification 2) end-to-end promopt training, involves representative patch selection 3) extensive validation experiment with Camelyon16, TCGA-NSCLC</p> <h2 id="method">Method</h2> <p>아래 그림처럼 세 step 으로 나누어 설명하고 있다.</p> <p>1) MIL classifier training 2) representative patch selection 3) promopt fine-tuning</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/bb7a88f5-20cb-47cf-b9bb-f501a9a13242/image.png" alt=""></p> <h3 id="attention-based-mil-classifier-with-frozen-feature-extractor">Attention-based MIL Classifier with Frozen Feature Extractor</h3> <p>attention-based MIL (ABMIL) 방식에서는 다음 세 식으로 요약할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/ff86cc84-9238-4b1e-9393-8acede2df4d1/image.png" alt=""></p> <p>F는 attention-weighted average of all patch features in WSI, </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/2d890433-ca8b-4b8a-bc3e-21a0d7e5bc63/image.png" alt=""></p> <p>w, V1, V2 는 learnable parameters in MIL classifier. (gated attention 이군요) 이를 다시 MIL classifier h 에 넣어서 최종 prediction of WSI label 을 만들고 CE loss 를 구한다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/72e9fe6d-6a73-4840-977c-8f2deafb33c8/image.png" alt=""></p> <p>minimize the prediction error 하는 방식으로 MIL classifier 를 학습함.</p> <h3 id="representative-patch-selection">Representative Patch Selection</h3> <ul> <li>배경: Camelyon16 관심있는 암 영역이 WSI 전체에서 10% 이하임. </li> <li>내용: select top-K patches with highest attention score (K=200)</li> <li>효과: reduce vast quantities of patches --&gt; enable end-to-end training</li> </ul> <h3 id="prompt-fine-tuning">Prompt Fine-tuning</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/90093a1f-2b2e-4fcd-914a-39b977c2a3b8/image.png" alt=""></p> <p>이쪽 부분이 잘 이해가 안가서, reference 하는 논문을 (꽤나 자세히) 읽어보고 왔다. 논문의 제목은 Exploring Visual Promopts for Adapting Large-Scale Models.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/b747b053-003a-4bb9-acc9-5933e5ae93c2/image.png" alt=""></p> <p>visual promopt 라는 random noise 같이 생긴 액자를 주목해서 보자. 핵심은 &quot;image perturbation&quot; 이다. 기존 backbone model 은 frozen 된 상태로, learnable parameter 로 만들어진 image perturbation 을 통해 task 자체를 reprogramming (like adversarial attack) 하고, frozen model 을 새로 programming 된 task 에 adapt 하도록 하는 것이다. 이 방식을 사용하면, 식으로 보면 조금 더 이해가 편하다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/03c155af-b0bb-4eb5-a676-0b8123fad434/image.png" alt=""></p> <p>task-specific visual promopt $v_{\phi}$ parameterized by $\phi$ 를 배우는 것이 목적이다. 이 논문에는 promopt design (액자식으로 하느냐-padding, random location, fixed location), output transformation (hard-coded mapping?) 과 같은 세부적인 내용도 담고 있으니 참고하도록 하자. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/99cfa9bb-3392-4ad4-9185-f483cc17cbfb/image.png" alt=""></p> <p>다시 보면 위 등장했던 그림이 더 이해가 될 것이다. 이번 논문에서는 저 $\phi$ 로 parameterize 된 visual prompt 를 여기서 어떻게 표현했는지 보자.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/49e80796-e802-4f9e-9658-b72eee047b3c/image.png" alt=""></p> <p>중간 단계의 feature map $f_i$ 를 ResNet block $g_i$ 에 통과시키 $p_i$ 라는 D dimension 을 가진 prompt vetor 를 얻고, 이 prompt vector 를 next block에서 다시 $f_{i+1}$과 channel-wise multiplication 을 거치게 된다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/59d52760-7848-4ba0-a518-2c00e3ade132/image.png" alt=""></p> <p>이렇게 표현함으로써, training process 에서 (feature extractor 는 frozen 된 채로) prompt block 에 해당하는 parameter 만으로, lightweight MIL classifier 가 end-to-end 방식으로 update 되는 것이다. </p> <h2 id="experiments">Experiments</h2> <h3 id="datasets">Datasets</h3> <ul> <li>Camelyon16: 256x256, 20x magnification, with average of 11556 patch per WSI, 399 WSI</li> <li>TCGA-NSCLC: 256x256, 20x magnification, with average 3089 patch per WSI, 1053 WSI</li> </ul> <h3 id="implementation-details">Implementation Details</h3> <p>promopt block 개수는 2-6개, top 200 patches 는 default</p> <h3 id="comparison-results">Comparison Results</h3> <p>sota MIL 인 DTFD, ABMIL 에 적용하여 실험함. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/af13ef04-99af-46b1-814f-6c3022d9b874/image.png" alt=""></p> <p>대부분의 실험에서 prompt fine-tuning 했을 때 성능이 더 향상되었다. 이 때 주목해야 할 점은 fine-tuning 은 cost 가 훨씬 많이 든다. 이는 demonstrate the advantage of visual prompt 라고 설명한다.</p> <h3 id="ablation-study">Ablation study</h3> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/6297017d-a355-4575-96f1-16a356143d04/image.png" alt=""></p> <p>Camelyon16 데이터에서, promopt block 개수, representative patch 개수를 달리하여 실험하였다. (a)를 보면, one prompt block 초과일 때 1%의 성능 향상을 보였다. (b) 에서는 K value 에 따라서, FT 방식에 비해 50% 이하의 GPU resource 를 사용하고도 (이는 본문에 나온 내용임) efficiency, effectiveness 향상이 있었다고 설명한다. - 몇개를 선택하는게 낫다 라는 statement 는 없음.</p> <h3 id="conclusion">Conclusion</h3> <p>novel promopt learning method to learn domain-specific knowledge transformation from ImageNet pre-trained model to pathological images</p> <p>돌려봐야겠다 그리고 sota 라고 언급한 DTFD-MIL 도...</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; April 11, 2023 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p></li> <li><h3> <a class="post-title" href="/blog/2023/tables/">displaying beautiful tables with Bootstrap Tables</a> </h3> <p>an example of how to use Bootstrap Tables</p> <p class="post-meta"> 3 min read &nbsp; &middot; &nbsp; March 20, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> &nbsp; &middot; &nbsp; <a href="/blog/tag/formatting"> <i class="fas fa-hashtag fa-sm"></i> formatting</a> &nbsp; <a href="/blog/tag/tables"> <i class="fas fa-hashtag fa-sm"></i> tables</a> &nbsp; &nbsp; &middot; &nbsp; <a href="/blog/category/sample-posts"> <i class="fas fa-tag fa-sm"></i> sample-posts</a> &nbsp; </p></li> <li><h3> <a class="post-title" href="/blog/2023/table-of-contents/">a post with table of contents</a> </h3> <p>an example of a blog post with table of contents</p> <p class="post-meta"> 3 min read &nbsp; &middot; &nbsp; March 20, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> &nbsp; &middot; &nbsp; <a href="/blog/tag/formatting"> <i class="fas fa-hashtag fa-sm"></i> formatting</a> &nbsp; <a href="/blog/tag/toc"> <i class="fas fa-hashtag fa-sm"></i> toc</a> &nbsp; &nbsp; &middot; &nbsp; <a href="/blog/category/sample-posts"> <i class="fas fa-tag fa-sm"></i> sample-posts</a> &nbsp; </p></li> <li><h3> <a class="post-title" href="https://velog.io/@jaeheon-lee/Paper-Review-Inductive-Representation-Learning-on-Large-Graphs" target="_blank">[Paper Review] Inductive Representation Learning on Large Graphs</a> <svg width="2rem" height="2rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p><h1 id="inductive-representation-learning-on-large-graphs">Inductive Representation Learning on Large Graphs</h1> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/d485b62f-aa08-4bc9-9b2b-beb95d7ff1dd/image.png" alt=""></p> <p>바로 이전 포스팅에서 다뤘던 대표적인 spectral method 인 GCN 는 edge 정보를 표현하지 못하고, scalable 하지 않다는 단점이 있었다. Message Passing Neural Networks (MPNN) 과 Graph Attention Networks (GAT) 는 edge feature 도 고려한 GNN architecture 를 사용한다. 그럼에도 node feature vector 는 entire neighbourhood 에 dependent 하기 때문에 아예 처음 본 graph 에 대해서는 적용이 불가능하다는 한계점이 존재한다. 이를 극복하기 위해 general inductive framework 인 GraphSAGE 가 등장한다. 이 방식은 지금까지도 쓰이는 graph 쪽 대표적인 논문이다.</p> <h2 id="introduction">Introduction</h2> <p>기존 방식 문제점</p> <ul> <li>only applied on fixed graph</li> <li>do not naturally generalize to unseen data</li> </ul> <p>Present work</p> <ul> <li>GraphSAGE (SAmple and aggreGatE)</li> <li>inductive node embedding</li> <li>learn the topological structure of each node&#39;s neighborhood</li> <li>distribution of node features in neighborhood</li> <li>instead of distinct embedding vector for each node, train aggregator functions</li> <li>design an unsupervised loss which could be simply replaced by supervised loss</li> </ul> <h2 id="proposed-method-graphsage">Proposed method: GraphSAgE</h2> <p>핵심 아이디어는 how to aggregate feature information from a node&#39;s local neighborhood 에 있다. 특히 K개의 aggregator function $AGGREGATE_k$ 와 그에 따른 weight matrices $W^k$ 는 각각 k번째 search depths 또는 k번째 다른 layer of model 에 propagate information 한다. </p> <h3 id="embedding-generation-ie-forward-propagation-algorithm">Embedding generation (i.e., forward propagation) algorithm</h3> <p>다음은 가장 핵심인 알고리즘이다. <img src="https://velog.velcdn.com/images/jaeheon-lee/post/1d39f5c9-4cf3-4715-8158-cd1b42390323/image.png" alt=""></p> <p>처음에 알고리즘만 보고 $AGGREGATE_k$ 에 대한 감이 안와서 코드를 찾아보았다. 요약한 내용은 다음과 같다.</p> <p>1) input 에 대해, hop (depth) 개수 만큼 loop 돌기 2) 각 loop 에서 target node vector는 weight 에 통과 후 저장 3) 각 loop 에서 target node 의 neighbor node vector 도 weight 에 통과 후, aggregation 후 weight 에 통과 후 저장 4) 저장된 두 vector 를 concatenation 후 저장 을 iteration 해서 마지막 1x32 vector 를 node 마다 계산하는 과정이다.</p> <p>setting: two layer setting, K=2</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/e696de86-9cce-426a-9671-f33d81d651ba/image.png" alt=""></p> <p>위 그림은 아래 블로그를 참고했다. 출처: <a href="https://antonsruberts.github.io/graph/graphsage/">https://antonsruberts.github.io/graph/graphsage/</a></p> <p>이 때, full neighborhood set을 사용하는 것 대신, computational footprint 를 각 batch 마다 일정하게 고정하기 위해서, 각 iteration 마다 different uniform sample 을 사용하였다.</p> <h3 id="learning-the-parameters-of-graphsage">Learning the parameters of GraphSAgE</h3> <p>fully unsupervised learning setting 에서 predictive representation 을 얻고자 다음과 같은 loss 를 사용했다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/ba0e4e96-917a-453e-bf55-fe247a7ba90d/image.png" alt=""></p> <p>이 때, 오른쪽 항 $P_n(v)$ 는 negative sampling distribution, Q 는 number of negative sample 이다. nearby node 간은 similar representation, disparatenode 간에는 highly distinct 할 것이라는 가정을 담고 있다. (흠..)</p> <p>model output 이 각 node 에 대한 feature vector 이기 때문에, unsupervised leanring 이 아니라 downstream task 등에서 fully supervised learning setting 방식을 이용해야 할 때, 단순히 loss 만 cross-entropy loss 등의 task-specific objective function 으로 바꿔주면 된다.</p> <h3 id="aggregator-architectures">Aggregator Architectures</h3> <p>algorithm 에는 $AGGREGATE_k$ 라 표현되어 있던 function 을 4가지 사용해서 비교 실험한다. graph data 에서는 node 간의 sequence 및 structure 가 중요하기 때문에, 이상적으로는 aggregating 하는 과정이 permutation invariance (논문에서는 symmetric) 해야 한다. </p> <ul> <li>Mean aggregator (mean and GCN)</li> </ul> <p>가장 기본적으로, 계산된 vector ${h_u^{k-1}, \forall u \in N(v)}$ 의 elementwise mean 을 취하면 mean aggregator 이다.</p> <ul> <li>GCN aggregator</li> </ul> <p>이를 변형하여, target node 와 $h_v^{k-1}$ 이웃 node ${h_u^{k-1}, \forall u \in N(v)}$ 를 concatenation 후 weight 를 씌우는 방식은, 기존 graph convolution 과 닮아있다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/03c407ff-b89f-4f3b-95c7-aebe72c40856/image.png" alt=""></p> <p>하지만, 기존 graph convolution 은 이전 hop 을 포함한 각 hop 을 concat 하는 방식이기에, 이전 버전과 명확히 다르고, 오히려 이는 &quot;skip connection&quot; between the different search depths or layers 라고 표현한다. (끄덕끄덕) </p> <ul> <li>LSTM aggregator</li> </ul> <p>aggregation 과정에서 RNN (LSTM) module 을 사용한 방식이다. 이는 larger expressive capability 를 가졌다는 장점이 있지만, not inherently symmetric 하다는 문제가 있다. (not permutation invariant) - 이는 어떤 방식에서든 bias 가 발생할 구실을 제공한다. </p> <ul> <li>Pooling Aggregator</li> </ul> <p>4번째 마지막 aggregator 는 trainble 하면서도 symmetric 하다. 다른 방식과는 다르게, aggregation 전에 각 이웃노드에 대해서 weight 를 통과 (MLP 통과) 시키고, elementwise 하게 max operation 을 취한다. </p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/018e09cf-c00b-4580-8659-51a6294b1674/image.png" alt=""></p> <p>이는 모델로 하여금 효율적으로 neighborhood set 의 different aspect 를 잡아내게 끔 한다. 또, max operator 대신 mean operator 도 사용해보았지만 no significant difference 를 보였다고 한다.</p> <h2 id="experiments--results">Experiments &amp; Results</h2> <p>다음 세가지 데이터셋에서 실험하였다.</p> <p>1) classifying academic papers into different subjects using the web of sicence citation dataset 2) classifying reddit posts as belonging to different communities 3) classisfying protein functions across various biological protein-protein iteration (PPI) graphs</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/652834f1-936e-4aed-8694-cc741199772f/image.png" alt=""></p> <p>속도 면에서도 GraphSAGE 를 사용했을 때 월등히 효율적이었다. DeepWalk 보다 100-500x 빨랐다. GraphSAGE variants 에서 K=2 setting 은 10-15%의 성능개선을 보였지만, 그 위로 넘어가면 갈수록 marginal return 과 함께 prohibitvely large factor of runtime 을 기록하였다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/1ddfe657-fd23-4f52-9749-c7d2fe4d3051/image.png" alt=""></p> <p>전반적으로, LSTM- and pool-based aggregator 방식을 차용한 모델에서 높은 성능을 보였다. 이 두 모델에 대해서 더 정량적인 평가를 위해 다른 aggregator 방식과 Wilcoxon Singed-Rank Test 를 진행했고, T-statistic 과 p-value 를 기록하였다. LSTM-, pool- aggregator 모두 GCN-based approach 와 비교했을 때, T=1, p=0.02 통계적으로 유의한 결과를 얻었다. </p> <h2 id="theoretical-analysis">Theoretical analysis</h2> <p>이번 섹션에서는 GraphSAGE 가 얼마나 graph structure 를 잘 배우는지 확인하기 위해 expressive capability 를 탐색하였다. 그 방식으로, clustering coefficient of a node 그 중에서도 the proportion of triangles that are closed within the node&#39;s 1-hop neighborhood 로 확인하였다. 그리고 다음 Theorem을 증명하였다.</p> <p><img src="https://velog.velcdn.com/images/jaeheon-lee/post/e8443861-e877-4a3f-8b74-6a70313be2f4/image.png" alt=""></p> <p>위 Theorem 은 어떤 graph 에 대해서, Algorithm 1 의 parameter setting이 존재하는데, st 이는 모든 node 의 feature 가 다르다는 가정하에, approximate clustering coefficient to an arbitrary precision 이 가능한 setting 이다. </p> <h2 id="conclusion">Conclusion</h2> <ul> <li>novel approach allowing embeddings effectively generated for unseen nodes</li> <li>outperform sota baselines</li> <li>effectively trade of performance and runtime by samplinbg node neighborhoods</li> <li>proved expressive capability about local graph structure</li> </ul> <p>오... 이런 algorithm 을 보고 elegant 하다 하는구나 2017년엔 무슨 일이 일어난걸까</p> </p> <p class="post-meta"> 1 min read &nbsp; &middot; &nbsp; February 24, 2023 &nbsp; &middot; &nbsp; jaeheon-lee </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p></li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/2/" tabindex="-1" aria-disabled="2">Newer</a> </li><li class="page-item "><a class="page-link" href="/blog/page/2/index.html" title="blog - page 2">2</a></li> <li class="page-item active"><a class="page-link" href="/blog/page/3/index.html" title="blog - page 3">3</a></li> <li class="page-item "><a class="page-link" href="/blog/page/4/index.html" title="blog - page 4">4</a></li> <li class="page-item "><a class="page-link" href="/blog/page/5/index.html" title="blog - page 5">5</a></li> <li class="page-item "><a class="page-link" href="/blog/page/6/index.html" title="blog - page 6">6</a></li> <li class="page-item "> <a class="page-link" href="/blog/page/4/">Older</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> &copy; Copyright 2023 JaeHeon Lee. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>